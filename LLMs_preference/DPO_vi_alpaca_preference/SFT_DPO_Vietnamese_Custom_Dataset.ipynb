{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a6c61671",
      "metadata": {
        "id": "a6c61671"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import torch\n",
        "\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from trl import DPOTrainer, DPOConfig, SFTConfig, SFTTrainer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c0ef4f3d",
      "metadata": {
        "id": "c0ef4f3d",
        "outputId": "6d440bf1-00d5-4df3-8ca0-4852d39149f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-12 20:59:36 [__init__.py:239] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 1024\n",
        "lora_rank = 16\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.4, \n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 2811,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "93425117",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight tensor([[ 0.0188,  0.0008,  0.0067,  ..., -0.0148,  0.0083, -0.0153],\n",
            "        [ 0.0084, -0.0100, -0.0185,  ..., -0.0154, -0.0217, -0.0120],\n",
            "        [-0.0092, -0.0051, -0.0143,  ...,  0.0147, -0.0104, -0.0125],\n",
            "        ...,\n",
            "        [ 0.0129,  0.0066, -0.0210,  ...,  0.0044,  0.0201, -0.0159],\n",
            "        [-0.0025,  0.0097,  0.0134,  ...,  0.0007, -0.0125,  0.0168],\n",
            "        [-0.0126, -0.0099,  0.0134,  ...,  0.0043, -0.0130, -0.0151]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# print the first tensors parameters\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da17e2f2",
      "metadata": {
        "id": "da17e2f2",
        "outputId": "a2ecaae6-00aa-484a-b079-970baec4a239"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "     train: Dataset({\n",
              "         features: ['id', 'question', 'chosen', 'rejected'],\n",
              "         num_rows: 65017\n",
              "     })\n",
              "     test: Dataset({\n",
              "         features: ['id', 'question', 'chosen', 'rejected'],\n",
              "         num_rows: 2000\n",
              "     })\n",
              " }),\n",
              " {'id': 'alpaca-7294',\n",
              "  'question': 'X√°c ƒë·ªãnh v√† s·ª≠a l·ªói ng·ªØ ph√°p.\\n\\nT√¥i ƒë√£ ƒëi ƒë·∫øn c·ª≠a h√†ng.',\n",
              "  'chosen': 'Kh√¥ng c√≥ l·ªói ng·ªØ ph√°p. C√¢u n√†y ƒë√£ ch√≠nh x√°c.',\n",
              "  'rejected': 'C√¢u n√†y kh√¥ng c√≥ l·ªói ng·ªØ ph√°p.'})"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset = load_dataset(\"thainq107/Vi-Alpaca-Preference\")\n",
        "dataset, dataset[\"train\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "2324e375",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'alpaca-14472',\n",
              " 'question': 'T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ \"th·ªùi trang mu·ªôn\".',\n",
              " 'chosen': '[Th√™m h√¨nh ·∫£nh ng∆∞·ªùi n·ªïi ti·∫øng m·∫∑c ƒë·ªì l·ªói th·ªùi, nh√¨n ch·∫≥ng ƒë·∫πp m·∫Øt, ph√π h·ª£p v·ªõi √Ω t∆∞·ªüng c·ªßa meme] \\n\\nH√£y tr√°nh th·ªùi trang mu·ªôn, v√¨ ƒë√¥i khi n√≥ c√≥ th·ªÉ l√† ƒëi·ªÅu t·ªìi t·ªá nh·∫•t m√† b·∫°n c√≥ th·ªÉ l√†m cho b·∫£n th√¢n m√¨nh.',\n",
              " 'rejected': 'Xin l·ªói, nh∆∞ng v√¨ t√¥i l√† m·ªôt tr√≠ tu·ªá nh√¢n t·∫°o v√† kh√¥ng c√≥ kh·∫£ nƒÉng th·ª±c hi·ªán c√°c t√°c v·ª• ƒë·ªì h·ªça, t√¥i kh√¥ng th·ªÉ t·∫°o ra m·ªôt thi·∫øt k·∫ø √°o ph√¥ng cho b·∫°n. Tuy nhi√™n, b·∫°n c√≥ th·ªÉ tham kh·∫£o c√°c trang web thi·∫øt k·∫ø √°o ph√¥ng tr·ª±c tuy·∫øn nh∆∞ Custom Ink, Printful ho·∫∑c Teespring ƒë·ªÉ t·∫°o ra thi·∫øt k·∫ø √°o ph√¥ng c·ªßa ri√™ng b·∫°n d·ª±a tr√™n ti·ªÉu bang ho·∫∑c qu·ªëc gia c·ªßa b·∫°n. Ch√∫c may m·∫Øn!'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959a1483",
      "metadata": {
        "id": "959a1483"
      },
      "source": [
        "## **SFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e13c22c9",
      "metadata": {
        "id": "e13c22c9"
      },
      "outputs": [],
      "source": [
        "def formatting_prompt_with_chat_template(example):\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": example[\"question\"]},\n",
        "        {\"role\": \"assistant\", \"content\": example[\"chosen\"]},\n",
        "    ]\n",
        "    prompt = tokenizer.apply_chat_template(\n",
        "        conversation, tokenize=False, add_generation_prompt=False\n",
        "    )\n",
        "    return prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa182d2",
      "metadata": {
        "id": "cfa182d2"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 200,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"save_strategy\": \"no\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_steps\": 200,\n",
        "    \"bf16\": True,\n",
        "}\n",
        "MAX_LENGTH = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f952c074",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a6604ba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "SFT_OUTPUT_DIR = f\"Qwen2.5-3B-Instruct-sft\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902f13d2",
      "metadata": {
        "id": "902f13d2",
        "outputId": "cb23a67f-b31c-4010-da44-4b1e6c2c84ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 65,017 | Num Epochs = 1 | Total steps = 4,063\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 29,933,568/3,000,000,000 (1.00% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtridungluong123\u001b[0m (\u001b[33mtridungluong123-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/binhphap5/DPO_preference_tuning/wandb/run-20250512_072958-sb3brag8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tridungluong123-university/huggingface/runs/sb3brag8' target=\"_blank\">Qwen2.5-3B-Instruct-sft</a></strong> to <a href='https://wandb.ai/tridungluong123-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tridungluong123-university/huggingface' target=\"_blank\">https://wandb.ai/tridungluong123-university/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tridungluong123-university/huggingface/runs/sb3brag8' target=\"_blank\">https://wandb.ai/tridungluong123-university/huggingface/runs/sb3brag8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4063' max='4063' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4063/4063 3:23:15, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.559500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.277200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.224200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.239700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.240200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.223200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.203500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.207000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.206600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.213500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.212900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.212700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.210900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.213100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.211500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.224800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.210900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.210200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4063, training_loss=1.2372856487524853, metrics={'train_runtime': 12201.5926, 'train_samples_per_second': 5.329, 'train_steps_per_second': 0.333, 'total_flos': 3.3485348060494234e+17, 'train_loss': 1.2372856487524853})"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sft_config = SFTConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       SFT_OUTPUT_DIR , \"max_seq_length\": MAX_LENGTH}\n",
        ")\n",
        "\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset['train'],\n",
        "    formatting_func=formatting_prompt_with_chat_template,\n",
        ")\n",
        "\n",
        "sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbb11da",
      "metadata": {
        "id": "1fbb11da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e146d72d8fc74940b9b9e78d980c5165",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.62k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e227f982ab54c3dbd37a62108361843",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/120M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a56425576374138939d2216d9a5c4bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4bfb8b93b3042fb93e58ac6f6493fce",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/binhphap5/Qwen2.5-3B-Instruct-sft/commit/68e5dc2edf2a5c0d24e828e7dc2f584146c1b8b4', commit_message='Qwen2.5-3B-instruct-sft-vi-alpaca', commit_description='', oid='68e5dc2edf2a5c0d24e828e7dc2f584146c1b8b4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/binhphap5/Qwen2.5-3B-Instruct-sft', endpoint='https://huggingface.co', repo_type='model', repo_id='binhphap5/Qwen2.5-3B-Instruct-sft'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sft_trainer.push_to_hub(\"Qwen2.5-3B-instruct-sft-vi-alpaca\", token=\"###\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf0ed59",
      "metadata": {
        "id": "aaf0ed59"
      },
      "source": [
        "## **DPO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6429f30",
      "metadata": {
        "id": "c6429f30"
      },
      "outputs": [],
      "source": [
        "def convert_to_conversational_preference_format(example):\n",
        "    return {\n",
        "        \"id\": example[\"id\"],\n",
        "        \"prompt\": [{\"role\": \"system\",\n",
        "                    \"content\": \"You are a helpful assistant.\"},\n",
        "                   {\"role\": \"user\",\n",
        "                    \"content\": example[\"question\"]}],\n",
        "        \"chosen\": [{\"role\": \"assistant\",\n",
        "                    \"content\": example[\"chosen\"]}],\n",
        "        \"rejected\": [{\"role\": \"assistant\",\n",
        "                      \"content\": example[\"rejected\"]}],\n",
        "    }\n",
        "\n",
        "dpo_dataset = dataset.map(convert_to_conversational_preference_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "827e926d",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.load_adapter(SFT_OUTPUT_DIR, is_trainable=True, adapter_name=\"dpo_full_adapter\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6f85e842",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "model.layers.0.self_attn.q_proj.lora_A.dpo_full_adapter.weight tensor([[ 0.0251,  0.0029,  0.0091,  ..., -0.0146,  0.0087, -0.0091],\n",
            "        [ 0.0020, -0.0121, -0.0184,  ..., -0.0148, -0.0165, -0.0157],\n",
            "        [-0.0021, -0.0035, -0.0135,  ...,  0.0136, -0.0131, -0.0096],\n",
            "        ...,\n",
            "        [ 0.0069,  0.0047, -0.0242,  ...,  0.0051,  0.0216, -0.0196],\n",
            "        [-0.0072,  0.0097,  0.0094,  ...,  0.0033, -0.0111,  0.0134],\n",
            "        [-0.0059, -0.0047,  0.0145,  ...,  0.0026, -0.0188, -0.0106]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# print the first tensors parameters\n",
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(name, param.data)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3cf73ee6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\n",
            "Response: ƒê√¢y l√† m·ªôt meme v·ªõi ti√™u ƒë·ªÅ \"Th·ªùi trang mu·ªôn\" v√† h√¨nh ·∫£nh c·ªßa m·ªôt ng∆∞·ªùi ƒë√†n √¥ng ƒëang m·∫∑c m·ªôt chi·∫øc √°o s∆° mi m√†u ƒëen, qu·∫ßn jeans v√† gi√†y cao g√≥t.\n",
            "\n",
            "Meme:\n",
            "\n",
            "Th·ªùi trang mu·ªôn\n",
            "\n",
            "Ng∆∞·ªùi ƒë√†n √¥ng n√†y ƒë√£ qu√° mu·ªôn ƒë·ªÉ ƒëi mua qu·∫ßn √°o m·ªõi cho m√¨nh. Anh ta ƒëang m·∫∑c m·ªôt chi·∫øc √°o s∆° mi m√†u ƒëen v√† qu·∫ßn jeans c≈© k·ªπ, nh∆∞ng v·∫´n c√≤n c·ªë g·∫Øng ƒë·ªÉ tr√¥ng sang tr·ªçng h∆°n b·∫±ng c√°ch ƒë·ªôi m·ªôt chi·∫øc m≈© cao c·∫•p v√† ƒëeo m·ªôt chi·∫øc t√∫i x√°ch ƒë·∫Øt ti·ªÅn.\n",
            "\n",
            "V·∫≠y l√† anh ta ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng ng∆∞·ªùi ƒë√†n √¥ng th·ªùi trang mu·ªôn nh·∫•t tr√™n ƒë∆∞·ªùng ph·ªë.\n"
          ]
        }
      ],
      "source": [
        "def get_model_response(model, tokenizer, instruction):\n",
        "    cur_conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": instruction}\n",
        "    ]\n",
        "    cur_input_prompt = tokenizer.apply_chat_template(\n",
        "        cur_conversation, add_generation_prompt=True, tokenize=True\n",
        "    )\n",
        "    cur_output_ids = model.generate(\n",
        "        input_ids=torch.LongTensor([cur_input_prompt]).to(model.device),\n",
        "        attention_mask=torch.ones(1, len(cur_input_prompt)).to(model.device),\n",
        "        max_new_tokens=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    cur_generated_ids = cur_output_ids[0][len(cur_input_prompt):]\n",
        "    return tokenizer.decode(cur_generated_ids, skip_special_tokens=True)\n",
        "\n",
        "instruction = \"T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\"\n",
        "response = get_model_response(model, tokenizer, instruction)\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "00b0f934",
      "metadata": {
        "id": "00b0f934",
        "outputId": "2ca61a52-cd5e-46b2-dd57-5e764632c115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'alpaca-14472',\n",
              " 'question': 'T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ \"th·ªùi trang mu·ªôn\".',\n",
              " 'chosen': [{'content': '[Th√™m h√¨nh ·∫£nh ng∆∞·ªùi n·ªïi ti·∫øng m·∫∑c ƒë·ªì l·ªói th·ªùi, nh√¨n ch·∫≥ng ƒë·∫πp m·∫Øt, ph√π h·ª£p v·ªõi √Ω t∆∞·ªüng c·ªßa meme] \\n\\nH√£y tr√°nh th·ªùi trang mu·ªôn, v√¨ ƒë√¥i khi n√≥ c√≥ th·ªÉ l√† ƒëi·ªÅu t·ªìi t·ªá nh·∫•t m√† b·∫°n c√≥ th·ªÉ l√†m cho b·∫£n th√¢n m√¨nh.',\n",
              "   'role': 'assistant'}],\n",
              " 'rejected': [{'content': 'Xin l·ªói, nh∆∞ng v√¨ t√¥i l√† m·ªôt tr√≠ tu·ªá nh√¢n t·∫°o v√† kh√¥ng c√≥ kh·∫£ nƒÉng th·ª±c hi·ªán c√°c t√°c v·ª• ƒë·ªì h·ªça, t√¥i kh√¥ng th·ªÉ t·∫°o ra m·ªôt thi·∫øt k·∫ø √°o ph√¥ng cho b·∫°n. Tuy nhi√™n, b·∫°n c√≥ th·ªÉ tham kh·∫£o c√°c trang web thi·∫øt k·∫ø √°o ph√¥ng tr·ª±c tuy·∫øn nh∆∞ Custom Ink, Printful ho·∫∑c Teespring ƒë·ªÉ t·∫°o ra thi·∫øt k·∫ø √°o ph√¥ng c·ªßa ri√™ng b·∫°n d·ª±a tr√™n ti·ªÉu bang ho·∫∑c qu·ªëc gia c·ªßa b·∫°n. Ch√∫c may m·∫Øn!',\n",
              "   'role': 'assistant'}],\n",
              " 'prompt': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ \"th·ªùi trang mu·ªôn\".',\n",
              "   'role': 'user'}]}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dpo_dataset['train'][1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "60e5b468",
      "metadata": {
        "id": "60e5b468"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 3,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 100,\n",
        "    \"max_steps\": 500,\n",
        "    \"save_strategy\": \"no\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_steps\": 20,\n",
        "    \"bf16\": True,\n",
        "}\n",
        "MAX_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "86f941ba",
      "metadata": {
        "id": "86f941ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtridungluong123\u001b[0m (\u001b[33mtridungluong123-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/binhphap5/DPO_preference_tuning/wandb/run-20250512_210623-3gug7f77</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/3gug7f77' target=\"_blank\">Qwen2.5-3B-4bit-dpo</a></strong> to <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference' target=\"_blank\">https://wandb.ai/tridungluong123-university/vi-alpaca-preference</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/3gug7f77' target=\"_blank\">https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/3gug7f77</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/3gug7f77?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fb79d0b5180>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use wandb\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"vi-alpaca-preference\",\n",
        "    name=\"Qwen2.5-3B-4bit-dpo\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "30ea761c",
      "metadata": {
        "id": "30ea761c",
        "outputId": "82f69757-753e-4b37-ea93-fc5e7e028f2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 65,017 | Num Epochs = 1 | Total steps = 500\n",
            "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 8 x 1) = 24\n",
            " \"-____-\"     Trainable parameters = 29,933,568/3,000,000,000 (1.00% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 1:41:26, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.300700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.161900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.189400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.157200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.19813651466369628, metrics={'train_runtime': 6097.8911, 'train_samples_per_second': 1.968, 'train_steps_per_second': 0.082, 'total_flos': 0.0, 'train_loss': 0.19813651466369628, 'epoch': 0.18456143588797122})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DPO_OUTPUT_DIR = f\"Qwen2.5-3B-Instruct-dpo\"\n",
        "dpo_args = DPOConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       DPO_OUTPUT_DIR, \"max_length\": MAX_LENGTH }\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    args=dpo_args,\n",
        "    train_dataset=dpo_dataset['train'],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "dpo_trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "ba24747e",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ec3aaa88",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\n",
            "Response: [Image: A person wearing outdated and mismatched clothes, with the caption \"Th·ªùi trang mu·ªôn\"]\n",
            "\n",
            "Explanation: The meme uses the phrase \"th·ªùi trang mu·ªôn\" which means \"late fashion\" or \"outdated fashion\". The image shows a person wearing clothes that are not in trend, creating a meme that satirizes those who choose to wear outdated fashion styles.\n"
          ]
        }
      ],
      "source": [
        "instruction = \"T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\"\n",
        "response = get_model_response(model, tokenizer, instruction)\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "c858ed0a",
      "metadata": {},
      "outputs": [],
      "source": [
        "dpo_trainer.save_model(DPO_OUTPUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "099a0942",
      "metadata": {
        "id": "099a0942"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e39eeedd195a4eb08e5a9e3bedc1533e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/116M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "303ea02ce0bf4fb38ce4108e4dc5035e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37d3dbd923c942c6b645c670841941f4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/6.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/binhphap5/Qwen2.5-3B-Instruct-dpo/commit/9d37307b02ae59c86cac29fb998fb000780c97b9', commit_message='Qwen2.5-3B-instruct-dpo-vi-alpaca', commit_description='', oid='9d37307b02ae59c86cac29fb998fb000780c97b9', pr_url=None, repo_url=RepoUrl('https://huggingface.co/binhphap5/Qwen2.5-3B-Instruct-dpo', endpoint='https://huggingface.co', repo_type='model', repo_id='binhphap5/Qwen2.5-3B-Instruct-dpo'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dpo_trainer.push_to_hub(\"Qwen2.5-3B-instruct-dpo-vi-alpaca\", token=\"###\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "general-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
