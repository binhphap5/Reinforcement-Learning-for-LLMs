{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "rQjtIKSWMz_z",
      "metadata": {
        "id": "rQjtIKSWMz_z"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1b9e81d4",
      "metadata": {
        "id": "1b9e81d4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, concatenate_datasets\n",
        "\n",
        "ds = load_dataset(\"uitnlp/vietnamese_students_feedback\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "28aa6ad5",
      "metadata": {
        "id": "28aa6ad5",
        "outputId": "09ecf6fb-ec67-43e7-ff81-5cd6facf5a6f"
      },
      "outputs": [],
      "source": [
        "# Concatenate all splits into a single dataset\n",
        "ds = concatenate_datasets([ds[\"train\"], ds[\"test\"], ds[\"validation\"]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cca38551",
      "metadata": {},
      "outputs": [],
      "source": [
        "ds=ds.remove_columns([\"sentiment\", \"topic\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1fa1a4c6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_sentence(example):\n",
        "    # Xóa khoảng cách trước dấu câu\n",
        "    example['sentence'] = re.sub(r'\\s+([.,!?;])', r'\\1', example['sentence'])\n",
        "    example['sentence'] = example['sentence'] + \" \"\n",
        "    return example\n",
        "\n",
        "ds = ds.map(clean_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e97ca06b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'thầy giảng bài hay, có nhiều bài tập ví dụ ngay trên lớp. '"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds[4]['sentence']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "8070bc40",
      "metadata": {
        "id": "8070bc40",
        "outputId": "497b9192-e119-4723-da9f-152b9eed6387"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"NlpHUST/gpt2-vietnamese\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "93ae6fc4",
      "metadata": {
        "id": "93ae6fc4",
        "outputId": "0c97c15f-52c4-4c3a-9d06-9e51f1dbc786"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "50258"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "58f4ae9f",
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer.pad_token = \"<pad>\"\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.eos_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f25573ab",
      "metadata": {
        "id": "f25573ab",
        "outputId": "7e4b6c62-87c7-4c1a-8995-e1cfd1d90a2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 50257, 50257)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer.pad_token_id, tokenizer.eos_token_id, tokenizer.bos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1fb39758",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "a96b4c685b8940628e1892e14b3a14dc",
            "887752a65791457088404b2b917bd04d"
          ]
        },
        "id": "1fb39758",
        "outputId": "db80d591-0d44-40e8-a11e-e8ad0accdf1f"
      },
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    return tokenizer(example[\"sentence\"])\n",
        "\n",
        "tokenized_ds = ds.map(\n",
        "    tokenize, remove_columns=[\"sentence\"], batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d4e80554",
      "metadata": {
        "id": "d4e80554",
        "outputId": "7739b210-5c51-4a7f-f167-bf3601583780"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 16175\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "54639d6e",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "f86f4a7a6860446292a71bce3310b6a0",
            "3967de32a9034bcfad3dc23fe296a60f"
          ]
        },
        "id": "54639d6e",
        "outputId": "6164ec26-eb4a-48ea-d9e4-61a6af21ebe1"
      },
      "outputs": [],
      "source": [
        "block_size = 128\n",
        "\n",
        "def group_texts(examples):\n",
        "    # concat input_ids\n",
        "    concatenated = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated[\"input_ids\"])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "\n",
        "    # split block_size\n",
        "    result = {\n",
        "        k: [concatenated[k][i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k in concatenated\n",
        "    }\n",
        "\n",
        "    # prepare labels\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_ds = tokenized_ds.map(group_texts, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "602f58a2",
      "metadata": {
        "id": "602f58a2",
        "outputId": "788862fc-036e-4486-ffd2-304d2d2b10d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 1941\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6cb2176f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# split train/test\n",
        "lm_ds = lm_ds.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "64e300a3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1746\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 195\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lm_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94vODCgaNB69",
      "metadata": {
        "id": "94vODCgaNB69"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "9dd76fe3",
      "metadata": {
        "id": "9dd76fe3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_29604\\2984512798.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-small-vietnamese-students-feedback\",\n",
        "    logging_dir=\"logs\",\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    learning_rate=7e-5,\n",
        "    num_train_epochs=30,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    logging_steps=50,\n",
        "    save_total_limit=1,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    load_best_model_at_end=True,\n",
        "    bf16=True,\n",
        "    gradient_checkpointing=False,\n",
        "    gradient_accumulation_steps=2,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_ds[\"train\"],\n",
        "    eval_dataset=lm_ds[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1qzgXdQmNH1y",
      "metadata": {
        "id": "1qzgXdQmNH1y"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "db0c77f7",
      "metadata": {
        "id": "db0c77f7",
        "outputId": "4e4b79b9-9c8a-437d-88a3-0ed200e15a79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='420' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [420/420 04:31, Epoch 30/30]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.915400</td>\n",
              "      <td>2.709089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.588200</td>\n",
              "      <td>2.614721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.483100</td>\n",
              "      <td>2.579675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.419300</td>\n",
              "      <td>2.562612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.369700</td>\n",
              "      <td>2.554899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.330900</td>\n",
              "      <td>2.551772</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.307300</td>\n",
              "      <td>2.551575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.289700</td>\n",
              "      <td>2.551540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=420, training_loss=2.4547282355172295, metrics={'train_runtime': 272.1543, 'train_samples_per_second': 192.464, 'train_steps_per_second': 1.543, 'total_flos': 3421619159040000.0, 'train_loss': 2.4547282355172295, 'epoch': 30.0})"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "d93ecf45",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "a7d90b61",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3397cae99aa4fd4a38122c2590847bb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/498M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c94b4f38d12e4fd19163de0a80e79fd1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac67549ce52f4ca7a5a585c914be7736",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "training_args.bin:   0%|          | 0.00/5.24k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/binhphap5/gpt2-small-vietnamese-students-feedback/commit/7a9cbdd7e58706b68a2c1d2d95838b4d8a2fa06c', commit_message='binhphap5/gpt2-small-vietnamese-students-feedback', commit_description='', oid='7a9cbdd7e58706b68a2c1d2d95838b4d8a2fa06c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/binhphap5/gpt2-small-vietnamese-students-feedback', endpoint='https://huggingface.co', repo_type='model', repo_id='binhphap5/gpt2-small-vietnamese-students-feedback'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# push to hub\n",
        "trainer.push_to_hub(\"binhphap5/gpt2-small-vietnamese-students-feedback\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6XPcE9v7NLae",
      "metadata": {
        "id": "6XPcE9v7NLae"
      },
      "source": [
        "## **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "e8542367",
      "metadata": {
        "id": "e8542367"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# model_name = \"binhphap5/gpt-small-c4\"\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name).to(\"cuda\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "659ffa73",
      "metadata": {
        "id": "659ffa73"
      },
      "outputs": [],
      "source": [
        "prompt = \"slide\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "52419802",
      "metadata": {
        "id": "52419802"
      },
      "outputs": [],
      "source": [
        "output = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "0778c933",
      "metadata": {
        "id": "0778c933",
        "outputId": "bf5a229b-0951-46c1-c332-0208000e314f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "slide quá khó hiểu, em đã học gần hết các phần trong sách rồi mà vẫn không hiểu, mong thầy có thể giải thích thêm các nội dung này. môn học nên kết hợp với thực hành ở nhà, thực hành trên máy để học lý thuyết, và thực hành trên lớp, thực hành trên máy nhiều khi còn nhàm chán. thầy dạy rất nhiệt tình và tâm huyết. thầy dạy rất nhiệt tình và tâm huyết, truyền đạt rất nhiều kinh nghiệm. cô dạy nhiệt tình, vui tính. giáo viên có thể chia sẻ nhiều kinh nghiệm dạy và học. cô giáo dễ thương, dạy dễ hiểu và\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(output[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4e11eab1",
      "metadata": {
        "id": "4e11eab1",
        "outputId": "2597e6b1-8f66-4f91-bcd4-e4ddef4dec03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.699362770034395"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "# Shift for labels (causal LM setting: predict token t+1 from token t)\n",
        "labels = output[:, 1:].clone()\n",
        "inputs = output[:, :-1].clone()\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Compute log softmax over vocabulary\n",
        "log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n",
        "# Gather log-probabilities corresponding to the labels\n",
        "selected_log_probs = log_probs.gather(2, labels.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "# Sum negative log probs → total NLL\n",
        "nll = -selected_log_probs.sum().item()\n",
        "num_tokens = labels.numel()\n",
        "perplexity = math.exp(nll / num_tokens)\n",
        "perplexity"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "tf-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
