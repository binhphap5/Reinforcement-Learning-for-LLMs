{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a6c61671",
      "metadata": {
        "id": "a6c61671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-13 17:28:08 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from trl import DPOTrainer, DPOConfig, SFTConfig, SFTTrainer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ef4f3d",
      "metadata": {
        "id": "c0ef4f3d",
        "outputId": "6d440bf1-00d5-4df3-8ca0-4852d39149f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-13 15:00:59 [__init__.py:239] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.4.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048\n",
        "lora_rank = 32\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.8, \n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 2811,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da17e2f2",
      "metadata": {
        "id": "da17e2f2",
        "outputId": "a2ecaae6-00aa-484a-b079-970baec4a239"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"5CD-AI/Vietnamese-Multi-turn-Chat-Alpaca\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2324e375",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['conversations', 'id'],\n",
              "     num_rows: 12697\n",
              " }),\n",
              " {'conversations': [{'from': 'human',\n",
              "    'value': 'Hãy chỉnh sửa câu này để ngắn gọn hơn mà không mất đi ý nghĩa: \"Trận đấu là một thất bại nặng nề mặc dù thực tế là cả đội đã tập luyện trong nhiều tuần.\"'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'Nhiều tuần huấn luyện của đội đã dẫn đến một thất bại nặng nề.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'Bạn có thể đề xuất một số chiến lược mà nhóm có thể sử dụng để cải thiện hiệu suất của họ trong trận đấu tiếp theo không?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'Chắc chắn, đây là một số chiến lược mà nhóm có thể sử dụng để cải thiện hiệu suất của mình trong trận đấu tiếp theo: 1. Phân tích trận đấu trước bằng cách xem lại cảnh quay trận đấu để xác định điểm yếu và các lĩnh vực cần cải thiện. 2. Tăng cường độ và sự tập trung của các buổi tập để đảm bảo mọi cầu thủ đều được chuẩn bị đầy đủ về thể chất và tinh thần. 3. Luyện tập những kỹ năng cụ thể cần cải thiện, chẳng hạn như sút bóng hoặc chuyền bóng chính xác. 4. Phát triển các chiến lược và chiến thuật của đội có tính đến điểm mạnh và điểm yếu của cả đội bạn và đối thủ. 5. Xây dựng sự gắn kết và giao tiếp trong nhóm để cải thiện sự cộng tác và làm việc nhóm trên thực địa. 6. Phát triển tinh thần dẻo dai và khả năng phục hồi để vượt qua thất bại và duy trì sự tập trung trong các tình huống áp lực cao. 7. Tăng cường thể chất để cải thiện sức bền và giảm mệt mỏi khi chơi game. 8. Nghỉ ngơi và phục hồi - đảm bảo rằng các cầu thủ được nghỉ ngơi đầy đủ và dinh dưỡng đầy đủ trước trận đấu để đảm bảo mọi người đều ở trạng thái tốt nhất.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'Cảm ơn những lời đề nghị. Bạn nghĩ chiến lược nào sẽ có tác động đáng kể nhất đến hiệu suất của nhóm?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'Là một mô hình ngôn ngữ AI, tôi không thể dự đoán chiến lược nào sẽ có tác động đáng kể nhất đến hiệu suất của nhóm vì mỗi nhóm là duy nhất và có thể yêu cầu các cách tiếp cận khác nhau để cải thiện hiệu suất của họ. Tuy nhiên, tôi có thể đề xuất rằng đội nên ưu tiên các chiến lược giải quyết những điểm yếu nhất của họ, đảm bảo rằng các cầu thủ của họ ở trạng thái thể chất và tinh thần tốt nhất, đồng thời xây dựng tinh thần đồng đội gắn kết để tận dụng tối đa mọi cầu thủ.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'Đó là một điểm tuyệt vời. Làm thế nào đội có thể cải thiện khả năng giao tiếp trên sân?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'Cải thiện giao tiếp trên thực địa là rất quan trọng để xây dựng một nhóm hiệu quả có thể làm việc cùng nhau và đạt được thành công. Dưới đây là một số chiến lược mà nhóm có thể sử dụng để cải thiện khả năng giao tiếp của họ trên hiện trường: 1. Thiết lập các giao thức liên lạc rõ ràng - thiết lập các tín hiệu và mã rõ ràng để mọi người trên hiện trường hiểu ý nghĩa của từng tín hiệu. 2. Khuyến khích giao tiếp cởi mở - người chơi nên được khuyến khích giao tiếp với nhau một cách tự do và cởi mở, không sợ bị chỉ trích hay bị trừng phạt. 3. Lặp lại thông tin chính - thông tin quan trọng, chẳng hạn như vị trí của người chơi, nên được lặp lại nhiều lần để đảm bảo rằng mọi người đều hiểu được. 4. Phát triển niềm tin - Người chơi nên tin tưởng lẫn nhau để đưa ra quyết định đúng đắn và giao tiếp hiệu quả, điều này sẽ giảm bớt sự nhầm lẫn và hiểu lầm trong quá trình chơi game. 5. Sử dụng ngôn ngữ tích cực - Ngôn ngữ tích cực có thể giúp thúc đẩy một môi trường hợp tác, hỗ trợ, khuyến khích giao tiếp và làm việc theo nhóm. 6. Thực hành giao tiếp trong quá trình tập luyện - Giao tiếp nên là một phần của các buổi tập luyện thường xuyên, để các cầu thủ quen với việc giao tiếp với nhau trong các trận đấu. Bằng cách tuân theo các chiến lược này, nhóm có thể cải thiện khả năng giao tiếp trên thực địa và làm việc cùng nhau hiệu quả hơn để đạt được mục tiêu của mình.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'Cảm ơn đã phản ứng chi tiết. Làm thế nào nhóm có thể theo dõi tiến trình của họ và đảm bảo rằng họ đang cải thiện theo thời gian?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'Đo lường tiến độ và phân tích kết quả có thể giúp nhóm đi đúng hướng và thực hiện các điều chỉnh để cải thiện hiệu suất của họ. Dưới đây là một số cách mà nhóm có thể theo dõi tiến trình của mình theo thời gian: 1. Đặt ra các mục tiêu cụ thể cần đạt được - việc có những mục tiêu rõ ràng và có thể đo lường được mà mọi người trong nhóm đều hiểu có thể giúp họ luôn có động lực và tập trung. 2. Thu thập dữ liệu và số liệu thống kê - dữ liệu có thể cung cấp phản hồi có giá trị về hiệu suất của đội, chẳng hạn như số bàn thắng ghi được, đường kiến \\u200b\\u200btạo và số cú sút trúng đích. 3. Sử dụng đánh giá của người chơi - đánh giá hiệu suất định kỳ có thể giúp xác định các lĩnh vực cần cải thiện và các lĩnh vực đang hoạt động tốt. 4. Phân tích cảnh quay trò chơi - sử dụng cảnh quay trò chơi để phân tích chuyển động, kiểu mẫu và giao tiếp của người chơi, nhóm có thể xác định các lĩnh vực cần cải thiện. 5. Theo dõi mức độ thể lực - theo dõi mức độ thể lực theo thời gian có thể giúp nhóm theo dõi sự cải thiện về thể lực, điều này có thể dẫn đến hiệu suất tốt hơn trên sân. 6. Kết hợp phản hồi từ huấn luyện viên và đồng đội - phản hồi từ huấn luyện viên và đồng đội có thể cung cấp những hiểu biết sâu sắc và ý tưởng có giá trị giúp cải thiện hiệu suất. Bằng cách liên tục theo dõi tiến trình của họ, nhóm có thể đảm bảo rằng họ đang tiến bộ theo thời gian và thực hiện các điều chỉnh về chiến lược cũng như đào tạo nếu cần thiết.'}],\n",
              "  'id': '19217'})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset, dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959a1483",
      "metadata": {
        "id": "959a1483"
      },
      "source": [
        "## **SFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3de56ea2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4a7d17b1647440b91ab307d0c53b4a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SYS_INSTRUCT = \"Bạn là một trợ lý AI thân thiện, hãy trả lời bằng tiếng Việt.\"\n",
        "\n",
        "def convert_to_chat_format(conversations):\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_INSTRUCT}]\n",
        "    for msg in conversations:\n",
        "        role = \"user\" if msg[\"from\"] == \"human\" else \"assistant\"\n",
        "        messages.append({\"role\": role, \"content\": msg[\"value\"]})\n",
        "    return messages\n",
        "\n",
        "def format_prompt(example):\n",
        "    messages = convert_to_chat_format(example[\"conversations\"])\n",
        "    return {\n",
        "        \"text\": tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
        "dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31615da3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "Bạn là một trợ lý AI thân thiện, hãy trả lời bằng tiếng Việt.<|im_end|>\n",
            "<|im_start|>user\n",
            "Hãy chỉnh sửa câu này để ngắn gọn hơn mà không mất đi ý nghĩa: \"Trận đấu là một thất bại nặng nề mặc dù thực tế là cả đội đã tập luyện trong nhiều tuần.\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Nhiều tuần huấn luyện của đội đã dẫn đến một thất bại nặng nề.<|im_end|>\n",
            "<|im_start|>user\n",
            "Bạn có thể đề xuất một số chiến lược mà nhóm có thể sử dụng để cải thiện hiệu suất của họ trong trận đấu tiếp theo không?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Chắc chắn, đây là một số chiến lược mà nhóm có thể sử dụng để cải thiện hiệu suất của mình trong trận đấu tiếp theo: 1. Phân tích trận đấu trước bằng cách xem lại cảnh quay trận đấu để xác định điểm yếu và các lĩnh vực cần cải thiện. 2. Tăng cường độ và sự tập trung của các buổi tập để đảm bảo mọi cầu thủ đều được chuẩn bị đầy đủ về thể chất và tinh thần. 3. Luyện tập những kỹ năng cụ thể cần cải thiện, chẳng hạn như sút bóng hoặc chuyền bóng chính xác. 4. Phát triển các chiến lược và chiến thuật của đội có tính đến điểm mạnh và điểm yếu của cả đội bạn và đối thủ. 5. Xây dựng sự gắn kết và giao tiếp trong nhóm để cải thiện sự cộng tác và làm việc nhóm trên thực địa. 6. Phát triển tinh thần dẻo dai và khả năng phục hồi để vượt qua thất bại và duy trì sự tập trung trong các tình huống áp lực cao. 7. Tăng cường thể chất để cải thiện sức bền và giảm mệt mỏi khi chơi game. 8. Nghỉ ngơi và phục hồi - đảm bảo rằng các cầu thủ được nghỉ ngơi đầy đủ và dinh dưỡng đầy đủ trước trận đấu để đảm bảo mọi người đều ở trạng thái tốt nhất.<|im_end|>\n",
            "<|im_start|>user\n",
            "Cảm ơn những lời đề nghị. Bạn nghĩ chiến lược nào sẽ có tác động đáng kể nhất đến hiệu suất của nhóm?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Là một mô hình ngôn ngữ AI, tôi không thể dự đoán chiến lược nào sẽ có tác động đáng kể nhất đến hiệu suất của nhóm vì mỗi nhóm là duy nhất và có thể yêu cầu các cách tiếp cận khác nhau để cải thiện hiệu suất của họ. Tuy nhiên, tôi có thể đề xuất rằng đội nên ưu tiên các chiến lược giải quyết những điểm yếu nhất của họ, đảm bảo rằng các cầu thủ của họ ở trạng thái thể chất và tinh thần tốt nhất, đồng thời xây dựng tinh thần đồng đội gắn kết để tận dụng tối đa mọi cầu thủ.<|im_end|>\n",
            "<|im_start|>user\n",
            "Đó là một điểm tuyệt vời. Làm thế nào đội có thể cải thiện khả năng giao tiếp trên sân?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Cải thiện giao tiếp trên thực địa là rất quan trọng để xây dựng một nhóm hiệu quả có thể làm việc cùng nhau và đạt được thành công. Dưới đây là một số chiến lược mà nhóm có thể sử dụng để cải thiện khả năng giao tiếp của họ trên hiện trường: 1. Thiết lập các giao thức liên lạc rõ ràng - thiết lập các tín hiệu và mã rõ ràng để mọi người trên hiện trường hiểu ý nghĩa của từng tín hiệu. 2. Khuyến khích giao tiếp cởi mở - người chơi nên được khuyến khích giao tiếp với nhau một cách tự do và cởi mở, không sợ bị chỉ trích hay bị trừng phạt. 3. Lặp lại thông tin chính - thông tin quan trọng, chẳng hạn như vị trí của người chơi, nên được lặp lại nhiều lần để đảm bảo rằng mọi người đều hiểu được. 4. Phát triển niềm tin - Người chơi nên tin tưởng lẫn nhau để đưa ra quyết định đúng đắn và giao tiếp hiệu quả, điều này sẽ giảm bớt sự nhầm lẫn và hiểu lầm trong quá trình chơi game. 5. Sử dụng ngôn ngữ tích cực - Ngôn ngữ tích cực có thể giúp thúc đẩy một môi trường hợp tác, hỗ trợ, khuyến khích giao tiếp và làm việc theo nhóm. 6. Thực hành giao tiếp trong quá trình tập luyện - Giao tiếp nên là một phần của các buổi tập luyện thường xuyên, để các cầu thủ quen với việc giao tiếp với nhau trong các trận đấu. Bằng cách tuân theo các chiến lược này, nhóm có thể cải thiện khả năng giao tiếp trên thực địa và làm việc cùng nhau hiệu quả hơn để đạt được mục tiêu của mình.<|im_end|>\n",
            "<|im_start|>user\n",
            "Cảm ơn đã phản ứng chi tiết. Làm thế nào nhóm có thể theo dõi tiến trình của họ và đảm bảo rằng họ đang cải thiện theo thời gian?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Đo lường tiến độ và phân tích kết quả có thể giúp nhóm đi đúng hướng và thực hiện các điều chỉnh để cải thiện hiệu suất của họ. Dưới đây là một số cách mà nhóm có thể theo dõi tiến trình của mình theo thời gian: 1. Đặt ra các mục tiêu cụ thể cần đạt được - việc có những mục tiêu rõ ràng và có thể đo lường được mà mọi người trong nhóm đều hiểu có thể giúp họ luôn có động lực và tập trung. 2. Thu thập dữ liệu và số liệu thống kê - dữ liệu có thể cung cấp phản hồi có giá trị về hiệu suất của đội, chẳng hạn như số bàn thắng ghi được, đường kiến ​​tạo và số cú sút trúng đích. 3. Sử dụng đánh giá của người chơi - đánh giá hiệu suất định kỳ có thể giúp xác định các lĩnh vực cần cải thiện và các lĩnh vực đang hoạt động tốt. 4. Phân tích cảnh quay trò chơi - sử dụng cảnh quay trò chơi để phân tích chuyển động, kiểu mẫu và giao tiếp của người chơi, nhóm có thể xác định các lĩnh vực cần cải thiện. 5. Theo dõi mức độ thể lực - theo dõi mức độ thể lực theo thời gian có thể giúp nhóm theo dõi sự cải thiện về thể lực, điều này có thể dẫn đến hiệu suất tốt hơn trên sân. 6. Kết hợp phản hồi từ huấn luyện viên và đồng đội - phản hồi từ huấn luyện viên và đồng đội có thể cung cấp những hiểu biết sâu sắc và ý tưởng có giá trị giúp cải thiện hiệu suất. Bằng cách liên tục theo dõi tiến trình của họ, nhóm có thể đảm bảo rằng họ đang tiến bộ theo thời gian và thực hiện các điều chỉnh về chiến lược cũng như đào tạo nếu cần thiết.<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "db3a6ccc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# data collator for causal LM\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cfa182d2",
      "metadata": {
        "id": "cfa182d2"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 100,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"save_strategy\": \"steps\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"bf16\": True,\n",
        "    \"report_to\": \"none\",\n",
        "}\n",
        "MAX_LENGTH = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f952c074",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a6604ba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "SFT_OUTPUT_DIR = f\"Qwen2.5-3B-Instruct-multi-conversation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "902f13d2",
      "metadata": {
        "id": "902f13d2",
        "outputId": "cb23a67f-b31c-4010-da44-4b1e6c2c84ec"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3c2cf1b9e38423fab5d4661ac952915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting train dataset to ChatML (num_proc=20):   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a2bd390b1ca47deb54108d56c1b7d27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset (num_proc=20):   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87f3397ad04748e895c7ac85e4f80fcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Packing train dataset (num_proc=20):   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,354 | Num Epochs = 1 | Total steps = 397\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 59,867,136/3,000,000,000 (2.00% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='397' max='397' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [397/397 1:08:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.357400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.229200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.207200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=397, training_loss=1.2482317677072674, metrics={'train_runtime': 4142.437, 'train_samples_per_second': 1.534, 'train_steps_per_second': 0.096, 'total_flos': 2.212532916804649e+17, 'train_loss': 1.2482317677072674})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sft_config = SFTConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       SFT_OUTPUT_DIR , \"max_seq_length\": MAX_LENGTH, \"packing\": True}\n",
        ")\n",
        "\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e2bb026d",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbb11da",
      "metadata": {
        "id": "1fbb11da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    './Qwen2.5-3B-Instruct-multi-conversation/checkpoint-397',\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1450f09",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.push_to_hub_merged(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-sft\",\n",
        "    commit_message=\"Merge lora weights to push to hub\",\n",
        "    token=\"???\"\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-sft\",\n",
        "    commit_message=\"Push tokenizer to hub\",\n",
        "    token=\"???\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b55aa677",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant reply: Chắc chắn! Dưới đây là một số chiến lược mà nhóm có thể sử dụng để cải thiện hiệu suất của họ trong trận đấu tiếp theo: 1. Xác định điểm yếu: Đánh giá lại trận đấu và xác định những điểm yếu của nhóm. Điều này có thể giúp nhóm xác định những điểm cần cải thiện và tập trung vào những điểm đó trong trận đấu tiếp theo. 2. Tăng cường tập luyện: Đặt ra mục tiêu tập luyện cụ thể và tập trung vào những kỹ năng cụ thể mà nhóm cần cải thiện. Điều này có thể bao gồm việc tập luyện\n"
          ]
        }
      ],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    top_k=10,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": (\n",
        "        \"Bạn là một trợ lý AI thân thiện, \"\n",
        "        \"hãy trả lời bằng tiếng Việt.\"\n",
        "    )},\n",
        "    {\"role\": \"user\", \"content\": (\n",
        "        \"Hãy chỉnh sửa câu này để ngắn gọn hơn mà không mất đi ý nghĩa: \"\n",
        "        \"\\\"Trận đấu là một thất bại nặng nề \"\n",
        "        \"mặc dù thực tế là cả đội đã tập luyện trong nhiều tuần.\\\"\"\n",
        "    )},\n",
        "    {\"role\": \"assistant\", \"content\": (\n",
        "        \"Nhiều tuần huấn luyện của đội đã dẫn đến một thất bại nặng nề.\"\n",
        "    )},\n",
        "    {\"role\": \"user\", \"content\": (\n",
        "        \"Bạn có thể đề xuất một số chiến lược mà \"\n",
        "        \"nhóm có thể sử dụng để cải thiện hiệu suất \"\n",
        "        \"của họ trong trận đấu tiếp theo không?\"\n",
        "    )}\n",
        "]\n",
        "\n",
        "chat_text = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False\n",
        ")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    chat_text,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda:0\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "if \"assistant\\n\" in output_text:\n",
        "    answer = output_text.split(\"assistant\\n\")[-1].strip()\n",
        "else:\n",
        "    answer = output_text.strip()\n",
        "\n",
        "print(\"Assistant reply:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf0ed59",
      "metadata": {
        "id": "aaf0ed59"
      },
      "source": [
        "## **DPO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e224218d",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"thainq107/Vi-Alpaca-Preference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83a28eaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef8bca4cb3f9404489bc83d554ffe00b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.4.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-sft\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "lora_rank = 16\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 2811,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2bcdbd06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import PatchDPOTrainer\n",
        "\n",
        "PatchDPOTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6429f30",
      "metadata": {
        "id": "c6429f30"
      },
      "outputs": [],
      "source": [
        "def convert_to_conversational_preference_format(example):\n",
        "    return {\n",
        "        \"id\": example[\"id\"],\n",
        "        \"prompt\": [{\"role\": \"system\",\n",
        "                    \"content\": \"Bạn là một trợ lý AI thân thiện, hãy trả lời bằng tiếng Việt.\"},\n",
        "                   {\"role\": \"user\",\n",
        "                    \"content\": example[\"question\"]}],\n",
        "        \"chosen\": [{\"role\": \"assistant\",\n",
        "                    \"content\": example[\"chosen\"]}],\n",
        "        \"rejected\": [{\"role\": \"assistant\",\n",
        "                      \"content\": example[\"rejected\"]}],\n",
        "    }\n",
        "\n",
        "dpo_dataset = dataset.map(convert_to_conversational_preference_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3cf73ee6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Tạo một meme bằng cách sử dụng cụm từ 'thời trang muộn'.\n",
            "Response: Tạo meme với câu \"thời trang muộn\" không khó, nhưng tôi không thể tạo hình ảnh hay video meme được vì tôi là trợ lý AI dựa trên văn bản. Tuy nhiên, tôi có thể mô tả cho bạn một meme có thể làm từ cụm từ này:\n",
            "\n",
            "Meme: \n",
            "- Hình nền là một người mặc quần áo thời trang hiện đại (ví dụ: áo khoác da, quần skinny jeans, giày cao gót).\n",
            "- Trong góc trái của hình nền, xuất hiện một người khác đang mặc đồ thời trang thập niên 90 (ví dụ: áo sơ mi rộng, quần jean looser, giày cao cổ).\n",
            "- Câu meme \"Thời trang muộn\" được in to và nổi bật ở giữa hai người.\n",
            "\n",
            "Meme này sẽ nói lên ý nghĩa rằng mặc thời trang muộn tức là mặc những thứ đã lỗi thời so với xu hướng hiện tại. \n",
            "\n",
            "Bạn có muốn tôi vẽ ra hình ảnh này không? Nếu có, tôi sẽ cố gắng thực hiện. \n",
            "\n",
            "(Nếu không, đây là ví dụ meme mà bạn có thể tạo ra: \"Thời trang muộn - Đừng mặc như người trước bạn\").\n"
          ]
        }
      ],
      "source": [
        "def get_model_response(model, tokenizer, instruction):\n",
        "    cur_conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"Bạn là một trợ lý AI thân thiện, hãy trả lời bằng tiếng Việt.\"},\n",
        "        {\"role\": \"user\", \"content\": instruction}\n",
        "    ]\n",
        "    cur_input_prompt = tokenizer.apply_chat_template(\n",
        "        cur_conversation, add_generation_prompt=True, tokenize=True\n",
        "    )\n",
        "    cur_output_ids = model.generate(\n",
        "        input_ids=torch.LongTensor([cur_input_prompt]).to(model.device),\n",
        "        attention_mask=torch.ones(1, len(cur_input_prompt)).to(model.device),\n",
        "        max_new_tokens=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    cur_generated_ids = cur_output_ids[0][len(cur_input_prompt):]\n",
        "    return tokenizer.decode(cur_generated_ids, skip_special_tokens=True)\n",
        "\n",
        "instruction = \"Tạo một meme bằng cách sử dụng cụm từ 'thời trang muộn'.\"\n",
        "response = get_model_response(model, tokenizer, instruction)\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "00b0f934",
      "metadata": {
        "id": "00b0f934",
        "outputId": "2ca61a52-cd5e-46b2-dd57-5e764632c115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'alpaca-14472',\n",
              " 'question': 'Tạo một meme bằng cách sử dụng cụm từ \"thời trang muộn\".',\n",
              " 'chosen': [{'content': '[Thêm hình ảnh người nổi tiếng mặc đồ lỗi thời, nhìn chẳng đẹp mắt, phù hợp với ý tưởng của meme] \\n\\nHãy tránh thời trang muộn, vì đôi khi nó có thể là điều tồi tệ nhất mà bạn có thể làm cho bản thân mình.',\n",
              "   'role': 'assistant'}],\n",
              " 'rejected': [{'content': 'Xin lỗi, nhưng vì tôi là một trí tuệ nhân tạo và không có khả năng thực hiện các tác vụ đồ họa, tôi không thể tạo ra một thiết kế áo phông cho bạn. Tuy nhiên, bạn có thể tham khảo các trang web thiết kế áo phông trực tuyến như Custom Ink, Printful hoặc Teespring để tạo ra thiết kế áo phông của riêng bạn dựa trên tiểu bang hoặc quốc gia của bạn. Chúc may mắn!',\n",
              "   'role': 'assistant'}],\n",
              " 'prompt': [{'content': 'Bạn là một trợ lý AI thân thiện, hãy trả lời bằng tiếng Việt.',\n",
              "   'role': 'system'},\n",
              "  {'content': 'Tạo một meme bằng cách sử dụng cụm từ \"thời trang muộn\".',\n",
              "   'role': 'user'}]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dpo_dataset['train'][1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "60e5b468",
      "metadata": {
        "id": "60e5b468"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 3,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 100,\n",
        "    \"max_steps\": 500,\n",
        "    \"save_strategy\": \"steps\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"bf16\": True,\n",
        "}\n",
        "MAX_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86f941ba",
      "metadata": {
        "id": "86f941ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtridungluong123\u001b[0m (\u001b[33mtridungluong123-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/binhphap5/hoctap/chat_model/wandb/run-20250513_172829-g5lv4658</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658' target=\"_blank\">Qwen2.5-3B-4bit-chat-dpo</a></strong> to <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference' target=\"_blank\">https://wandb.ai/tridungluong123-university/vi-alpaca-preference</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658' target=\"_blank\">https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f9d5aebd390>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use wandb\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"vi-alpaca-preference\",\n",
        "    name=\"Qwen2.5-3B-4bit-chat-dpo\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "30ea761c",
      "metadata": {
        "id": "30ea761c",
        "outputId": "82f69757-753e-4b37-ea93-fc5e7e028f2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 65,017 | Num Epochs = 1 | Total steps = 500\n",
            "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 8 x 1) = 24\n",
            " \"-____-\"     Trainable parameters = 29,933,568/3,000,000,000 (1.00% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 1:09:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.353300</td>\n",
              "      <td>-1.060731</td>\n",
              "      <td>-4.226553</td>\n",
              "      <td>0.787500</td>\n",
              "      <td>3.165823</td>\n",
              "      <td>-211.543243</td>\n",
              "      <td>-358.595703</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>-2.453786</td>\n",
              "      <td>-10.323666</td>\n",
              "      <td>0.874583</td>\n",
              "      <td>7.869880</td>\n",
              "      <td>-227.215591</td>\n",
              "      <td>-413.576630</td>\n",
              "      <td>nan</td>\n",
              "      <td>-3.680350</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.174300</td>\n",
              "      <td>-2.679446</td>\n",
              "      <td>-11.215009</td>\n",
              "      <td>0.884583</td>\n",
              "      <td>8.535563</td>\n",
              "      <td>-234.342728</td>\n",
              "      <td>-424.946289</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.176000</td>\n",
              "      <td>-2.190362</td>\n",
              "      <td>-11.647231</td>\n",
              "      <td>0.886250</td>\n",
              "      <td>9.456869</td>\n",
              "      <td>-230.944473</td>\n",
              "      <td>-433.805481</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.162100</td>\n",
              "      <td>-2.011829</td>\n",
              "      <td>-12.378806</td>\n",
              "      <td>0.889583</td>\n",
              "      <td>10.366978</td>\n",
              "      <td>-227.305801</td>\n",
              "      <td>-440.325043</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.2120833625793457, metrics={'train_runtime': 4202.3252, 'train_samples_per_second': 2.856, 'train_steps_per_second': 0.119, 'total_flos': 0.0, 'train_loss': 0.2120833625793457, 'epoch': 0.18456143588797122})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DPO_OUTPUT_DIR = f\"Qwen2.5-3B-Instruct-chat-dpo\"\n",
        "dpo_args = DPOConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       DPO_OUTPUT_DIR, \"max_length\": MAX_LENGTH }\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model = None,\n",
        "    args=dpo_args,\n",
        "    train_dataset=dpo_dataset['train'],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "dpo_trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba24747e",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ec3aaa88",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: Tạo một meme bằng cách sử dụng cụm từ 'thời trang muộn'.\n",
            "Response: Tạo meme với cụm từ 'thời trang muộn' không phải là khả thi bằng lời văn đơn giản vì meme thường cần hình ảnh hoặc video. Tuy nhiên, tôi có thể tạo ra một meme bằng cách sử dụng câu này:\n",
            "\n",
            "[Ảnh: Một người mặc đồ thời trang cũ, nhưng đang đứng trước gương, phía sau là dòng chữ \"Thời trang muộn\".]\n",
            "\n",
            "Meme này có thể sẽ gây cười khi thấy một người cố gắng cập nhật thời trang nhưng đã quá muộn so với xu hướng hiện tại. \n",
            "\n",
            "Hoặc nếu bạn muốn một meme ngắn gọn hơn, có thể như sau: \n",
            "[Ảnh: Một người phụ nữ đang thử quần áo mới, phía sau là dòng chữ \"Thời trang muộn\" với hình ảnh của một người đàn ông đang ăn bánh ngọt.] \n",
            "\n",
            "Trong cả hai meme này, câu \"thời trang muộn\" được dùng để tạo ra sự khác biệt và tạo ra một ý tưởng hài hước. \n",
            "\n",
            "Nhưng nhớ rằng meme tốt nhất thường dựa trên một hình ảnh hoặc video, nên nếu bạn muốn tạo meme, bạn nên tìm kiếm một hình ảnh phù hợp.\n"
          ]
        }
      ],
      "source": [
        "instruction = \"Tạo một meme bằng cách sử dụng cụm từ 'thời trang muộn'.\"\n",
        "response = get_model_response(model, tokenizer, instruction)\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "099a0942",
      "metadata": {
        "id": "099a0942"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    './Qwen2.5-3B-Instruct-chat-dpo/checkpoint-500',\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model.push_to_hub_merged(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-RLHF\",\n",
        "    commit_message=\"Merge lora weights to push to hub\",\n",
        "    token=\"???\"\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-RLHF\",\n",
        "    commit_message=\"Push tokenizer to hub\",\n",
        "    token=\"???\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c27076a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "vllm serve binhphap5/Qwen2.5-3B-Instruct-Chat-RLHF \\\n",
        "--api-key binhphap5 \\\n",
        "--port 8000 \\\n",
        "--quantization bitsandbytes \\\n",
        "--enable-prefix-caching \\\n",
        "--swap-space 16 \\\n",
        "--gpu-memory-utilization 0.9 \\\n",
        "--disable-log-requests \\\n",
        "--max-model-len 2048 \n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "general-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
