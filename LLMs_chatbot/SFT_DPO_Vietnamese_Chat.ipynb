{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "a6c61671",
      "metadata": {
        "id": "a6c61671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-13 17:28:08 [__init__.py:239] Automatically detected platform cuda.\n"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from trl import DPOTrainer, DPOConfig, SFTConfig, SFTTrainer\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0ef4f3d",
      "metadata": {
        "id": "c0ef4f3d",
        "outputId": "6d440bf1-00d5-4df3-8ca0-4852d39149f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "INFO 05-13 15:00:59 [__init__.py:239] Automatically detected platform cuda.\n",
            "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.4.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "max_seq_length = 2048\n",
        "lora_rank = 32\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"Qwen/Qwen2.5-3B-Instruct\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    load_in_4bit = True,\n",
        "    max_lora_rank = lora_rank,\n",
        "    gpu_memory_utilization = 0.8, \n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 2811,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da17e2f2",
      "metadata": {
        "id": "da17e2f2",
        "outputId": "a2ecaae6-00aa-484a-b079-970baec4a239"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"5CD-AI/Vietnamese-Multi-turn-Chat-Alpaca\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2324e375",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Dataset({\n",
              "     features: ['conversations', 'id'],\n",
              "     num_rows: 12697\n",
              " }),\n",
              " {'conversations': [{'from': 'human',\n",
              "    'value': 'H√£y ch·ªânh s·ª≠a c√¢u n√†y ƒë·ªÉ ng·∫Øn g·ªçn h∆°n m√† kh√¥ng m·∫•t ƒëi √Ω nghƒ©a: \"Tr·∫≠n ƒë·∫•u l√† m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ m·∫∑c d√π th·ª±c t·∫ø l√† c·∫£ ƒë·ªôi ƒë√£ t·∫≠p luy·ªán trong nhi·ªÅu tu·∫ßn.\"'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'Nhi·ªÅu tu·∫ßn hu·∫•n luy·ªán c·ªßa ƒë·ªôi ƒë√£ d·∫´n ƒë·∫øn m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'B·∫°n c√≥ th·ªÉ ƒë·ªÅ xu·∫•t m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç trong tr·∫≠n ƒë·∫•u ti·∫øp theo kh√¥ng?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'Ch·∫Øc ch·∫Øn, ƒë√¢y l√† m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa m√¨nh trong tr·∫≠n ƒë·∫•u ti·∫øp theo: 1. Ph√¢n t√≠ch tr·∫≠n ƒë·∫•u tr∆∞·ªõc b·∫±ng c√°ch xem l·∫°i c·∫£nh quay tr·∫≠n ƒë·∫•u ƒë·ªÉ x√°c ƒë·ªãnh ƒëi·ªÉm y·∫øu v√† c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán. 2. TƒÉng c∆∞·ªùng ƒë·ªô v√† s·ª± t·∫≠p trung c·ªßa c√°c bu·ªïi t·∫≠p ƒë·ªÉ ƒë·∫£m b·∫£o m·ªçi c·∫ßu th·ªß ƒë·ªÅu ƒë∆∞·ª£c chu·∫©n b·ªã ƒë·∫ßy ƒë·ªß v·ªÅ th·ªÉ ch·∫•t v√† tinh th·∫ßn. 3. Luy·ªán t·∫≠p nh·ªØng k·ªπ nƒÉng c·ª• th·ªÉ c·∫ßn c·∫£i thi·ªán, ch·∫≥ng h·∫°n nh∆∞ s√∫t b√≥ng ho·∫∑c chuy·ªÅn b√≥ng ch√≠nh x√°c. 4. Ph√°t tri·ªÉn c√°c chi·∫øn l∆∞·ª£c v√† chi·∫øn thu·∫≠t c·ªßa ƒë·ªôi c√≥ t√≠nh ƒë·∫øn ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu c·ªßa c·∫£ ƒë·ªôi b·∫°n v√† ƒë·ªëi th·ªß. 5. X√¢y d·ª±ng s·ª± g·∫Øn k·∫øt v√† giao ti·∫øp trong nh√≥m ƒë·ªÉ c·∫£i thi·ªán s·ª± c·ªông t√°c v√† l√†m vi·ªác nh√≥m tr√™n th·ª±c ƒë·ªãa. 6. Ph√°t tri·ªÉn tinh th·∫ßn d·∫ªo dai v√† kh·∫£ nƒÉng ph·ª•c h·ªìi ƒë·ªÉ v∆∞·ª£t qua th·∫•t b·∫°i v√† duy tr√¨ s·ª± t·∫≠p trung trong c√°c t√¨nh hu·ªëng √°p l·ª±c cao. 7. TƒÉng c∆∞·ªùng th·ªÉ ch·∫•t ƒë·ªÉ c·∫£i thi·ªán s·ª©c b·ªÅn v√† gi·∫£m m·ªát m·ªèi khi ch∆°i game. 8. Ngh·ªâ ng∆°i v√† ph·ª•c h·ªìi - ƒë·∫£m b·∫£o r·∫±ng c√°c c·∫ßu th·ªß ƒë∆∞·ª£c ngh·ªâ ng∆°i ƒë·∫ßy ƒë·ªß v√† dinh d∆∞·ª°ng ƒë·∫ßy ƒë·ªß tr∆∞·ªõc tr·∫≠n ƒë·∫•u ƒë·ªÉ ƒë·∫£m b·∫£o m·ªçi ng∆∞·ªùi ƒë·ªÅu ·ªü tr·∫°ng th√°i t·ªët nh·∫•t.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'C·∫£m ∆°n nh·ªØng l·ªùi ƒë·ªÅ ngh·ªã. B·∫°n nghƒ© chi·∫øn l∆∞·ª£c n√†o s·∫Ω c√≥ t√°c ƒë·ªông ƒë√°ng k·ªÉ nh·∫•t ƒë·∫øn hi·ªáu su·∫•t c·ªßa nh√≥m?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'L√† m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ AI, t√¥i kh√¥ng th·ªÉ d·ª± ƒëo√°n chi·∫øn l∆∞·ª£c n√†o s·∫Ω c√≥ t√°c ƒë·ªông ƒë√°ng k·ªÉ nh·∫•t ƒë·∫øn hi·ªáu su·∫•t c·ªßa nh√≥m v√¨ m·ªói nh√≥m l√† duy nh·∫•t v√† c√≥ th·ªÉ y√™u c·∫ßu c√°c c√°ch ti·∫øp c·∫≠n kh√°c nhau ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç. Tuy nhi√™n, t√¥i c√≥ th·ªÉ ƒë·ªÅ xu·∫•t r·∫±ng ƒë·ªôi n√™n ∆∞u ti√™n c√°c chi·∫øn l∆∞·ª£c gi·∫£i quy·∫øt nh·ªØng ƒëi·ªÉm y·∫øu nh·∫•t c·ªßa h·ªç, ƒë·∫£m b·∫£o r·∫±ng c√°c c·∫ßu th·ªß c·ªßa h·ªç ·ªü tr·∫°ng th√°i th·ªÉ ch·∫•t v√† tinh th·∫ßn t·ªët nh·∫•t, ƒë·ªìng th·ªùi x√¢y d·ª±ng tinh th·∫ßn ƒë·ªìng ƒë·ªôi g·∫Øn k·∫øt ƒë·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa m·ªçi c·∫ßu th·ªß.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'ƒê√≥ l√† m·ªôt ƒëi·ªÉm tuy·ªát v·ªùi. L√†m th·∫ø n√†o ƒë·ªôi c√≥ th·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng giao ti·∫øp tr√™n s√¢n?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'C·∫£i thi·ªán giao ti·∫øp tr√™n th·ª±c ƒë·ªãa l√† r·∫•t quan tr·ªçng ƒë·ªÉ x√¢y d·ª±ng m·ªôt nh√≥m hi·ªáu qu·∫£ c√≥ th·ªÉ l√†m vi·ªác c√πng nhau v√† ƒë·∫°t ƒë∆∞·ª£c th√†nh c√¥ng. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng giao ti·∫øp c·ªßa h·ªç tr√™n hi·ªán tr∆∞·ªùng: 1. Thi·∫øt l·∫≠p c√°c giao th·ª©c li√™n l·∫°c r√µ r√†ng - thi·∫øt l·∫≠p c√°c t√≠n hi·ªáu v√† m√£ r√µ r√†ng ƒë·ªÉ m·ªçi ng∆∞·ªùi tr√™n hi·ªán tr∆∞·ªùng hi·ªÉu √Ω nghƒ©a c·ªßa t·ª´ng t√≠n hi·ªáu. 2. Khuy·∫øn kh√≠ch giao ti·∫øp c·ªüi m·ªü - ng∆∞·ªùi ch∆°i n√™n ƒë∆∞·ª£c khuy·∫øn kh√≠ch giao ti·∫øp v·ªõi nhau m·ªôt c√°ch t·ª± do v√† c·ªüi m·ªü, kh√¥ng s·ª£ b·ªã ch·ªâ tr√≠ch hay b·ªã tr·ª´ng ph·∫°t. 3. L·∫∑p l·∫°i th√¥ng tin ch√≠nh - th√¥ng tin quan tr·ªçng, ch·∫≥ng h·∫°n nh∆∞ v·ªã tr√≠ c·ªßa ng∆∞·ªùi ch∆°i, n√™n ƒë∆∞·ª£c l·∫∑p l·∫°i nhi·ªÅu l·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng m·ªçi ng∆∞·ªùi ƒë·ªÅu hi·ªÉu ƒë∆∞·ª£c. 4. Ph√°t tri·ªÉn ni·ªÅm tin - Ng∆∞·ªùi ch∆°i n√™n tin t∆∞·ªüng l·∫´n nhau ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh ƒë√∫ng ƒë·∫Øn v√† giao ti·∫øp hi·ªáu qu·∫£, ƒëi·ªÅu n√†y s·∫Ω gi·∫£m b·ªõt s·ª± nh·∫ßm l·∫´n v√† hi·ªÉu l·∫ßm trong qu√° tr√¨nh ch∆°i game. 5. S·ª≠ d·ª•ng ng√¥n ng·ªØ t√≠ch c·ª±c - Ng√¥n ng·ªØ t√≠ch c·ª±c c√≥ th·ªÉ gi√∫p th√∫c ƒë·∫©y m·ªôt m√¥i tr∆∞·ªùng h·ª£p t√°c, h·ªó tr·ª£, khuy·∫øn kh√≠ch giao ti·∫øp v√† l√†m vi·ªác theo nh√≥m. 6. Th·ª±c h√†nh giao ti·∫øp trong qu√° tr√¨nh t·∫≠p luy·ªán - Giao ti·∫øp n√™n l√† m·ªôt ph·∫ßn c·ªßa c√°c bu·ªïi t·∫≠p luy·ªán th∆∞·ªùng xuy√™n, ƒë·ªÉ c√°c c·∫ßu th·ªß quen v·ªõi vi·ªác giao ti·∫øp v·ªõi nhau trong c√°c tr·∫≠n ƒë·∫•u. B·∫±ng c√°ch tu√¢n theo c√°c chi·∫øn l∆∞·ª£c n√†y, nh√≥m c√≥ th·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng giao ti·∫øp tr√™n th·ª±c ƒë·ªãa v√† l√†m vi·ªác c√πng nhau hi·ªáu qu·∫£ h∆°n ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ª•c ti√™u c·ªßa m√¨nh.'},\n",
              "   {'from': 'human',\n",
              "    'value': 'C·∫£m ∆°n ƒë√£ ph·∫£n ·ª©ng chi ti·∫øt. L√†m th·∫ø n√†o nh√≥m c√≥ th·ªÉ theo d√µi ti·∫øn tr√¨nh c·ªßa h·ªç v√† ƒë·∫£m b·∫£o r·∫±ng h·ªç ƒëang c·∫£i thi·ªán theo th·ªùi gian?'},\n",
              "   {'from': 'gpt',\n",
              "    'value': 'ƒêo l∆∞·ªùng ti·∫øn ƒë·ªô v√† ph√¢n t√≠ch k·∫øt qu·∫£ c√≥ th·ªÉ gi√∫p nh√≥m ƒëi ƒë√∫ng h∆∞·ªõng v√† th·ª±c hi·ªán c√°c ƒëi·ªÅu ch·ªânh ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë c√°ch m√† nh√≥m c√≥ th·ªÉ theo d√µi ti·∫øn tr√¨nh c·ªßa m√¨nh theo th·ªùi gian: 1. ƒê·∫∑t ra c√°c m·ª•c ti√™u c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c - vi·ªác c√≥ nh·ªØng m·ª•c ti√™u r√µ r√†ng v√† c√≥ th·ªÉ ƒëo l∆∞·ªùng ƒë∆∞·ª£c m√† m·ªçi ng∆∞·ªùi trong nh√≥m ƒë·ªÅu hi·ªÉu c√≥ th·ªÉ gi√∫p h·ªç lu√¥n c√≥ ƒë·ªông l·ª±c v√† t·∫≠p trung. 2. Thu th·∫≠p d·ªØ li·ªáu v√† s·ªë li·ªáu th·ªëng k√™ - d·ªØ li·ªáu c√≥ th·ªÉ cung c·∫•p ph·∫£n h·ªìi c√≥ gi√° tr·ªã v·ªÅ hi·ªáu su·∫•t c·ªßa ƒë·ªôi, ch·∫≥ng h·∫°n nh∆∞ s·ªë b√†n th·∫Øng ghi ƒë∆∞·ª£c, ƒë∆∞·ªùng ki·∫øn \\u200b\\u200bt·∫°o v√† s·ªë c√∫ s√∫t tr√∫ng ƒë√≠ch. 3. S·ª≠ d·ª•ng ƒë√°nh gi√° c·ªßa ng∆∞·ªùi ch∆°i - ƒë√°nh gi√° hi·ªáu su·∫•t ƒë·ªãnh k·ª≥ c√≥ th·ªÉ gi√∫p x√°c ƒë·ªãnh c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán v√† c√°c lƒ©nh v·ª±c ƒëang ho·∫°t ƒë·ªông t·ªët. 4. Ph√¢n t√≠ch c·∫£nh quay tr√≤ ch∆°i - s·ª≠ d·ª•ng c·∫£nh quay tr√≤ ch∆°i ƒë·ªÉ ph√¢n t√≠ch chuy·ªÉn ƒë·ªông, ki·ªÉu m·∫´u v√† giao ti·∫øp c·ªßa ng∆∞·ªùi ch∆°i, nh√≥m c√≥ th·ªÉ x√°c ƒë·ªãnh c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán. 5. Theo d√µi m·ª©c ƒë·ªô th·ªÉ l·ª±c - theo d√µi m·ª©c ƒë·ªô th·ªÉ l·ª±c theo th·ªùi gian c√≥ th·ªÉ gi√∫p nh√≥m theo d√µi s·ª± c·∫£i thi·ªán v·ªÅ th·ªÉ l·ª±c, ƒëi·ªÅu n√†y c√≥ th·ªÉ d·∫´n ƒë·∫øn hi·ªáu su·∫•t t·ªët h∆°n tr√™n s√¢n. 6. K·∫øt h·ª£p ph·∫£n h·ªìi t·ª´ hu·∫•n luy·ªán vi√™n v√† ƒë·ªìng ƒë·ªôi - ph·∫£n h·ªìi t·ª´ hu·∫•n luy·ªán vi√™n v√† ƒë·ªìng ƒë·ªôi c√≥ th·ªÉ cung c·∫•p nh·ªØng hi·ªÉu bi·∫øt s√¢u s·∫Øc v√† √Ω t∆∞·ªüng c√≥ gi√° tr·ªã gi√∫p c·∫£i thi·ªán hi·ªáu su·∫•t. B·∫±ng c√°ch li√™n t·ª•c theo d√µi ti·∫øn tr√¨nh c·ªßa h·ªç, nh√≥m c√≥ th·ªÉ ƒë·∫£m b·∫£o r·∫±ng h·ªç ƒëang ti·∫øn b·ªô theo th·ªùi gian v√† th·ª±c hi·ªán c√°c ƒëi·ªÅu ch·ªânh v·ªÅ chi·∫øn l∆∞·ª£c c≈©ng nh∆∞ ƒë√†o t·∫°o n·∫øu c·∫ßn thi·∫øt.'}],\n",
              "  'id': '19217'})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset, dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "959a1483",
      "metadata": {
        "id": "959a1483"
      },
      "source": [
        "## **SFT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3de56ea2",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4a7d17b1647440b91ab307d0c53b4a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "SYS_INSTRUCT = \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\"\n",
        "\n",
        "def convert_to_chat_format(conversations):\n",
        "    messages = [{\"role\": \"system\", \"content\": SYS_INSTRUCT}]\n",
        "    for msg in conversations:\n",
        "        role = \"user\" if msg[\"from\"] == \"human\" else \"assistant\"\n",
        "        messages.append({\"role\": role, \"content\": msg[\"value\"]})\n",
        "    return messages\n",
        "\n",
        "def format_prompt(example):\n",
        "    messages = convert_to_chat_format(example[\"conversations\"])\n",
        "    return {\n",
        "        \"text\": tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "    }\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=max_seq_length,\n",
        "    )\n",
        "\n",
        "dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
        "dataset = dataset.map(tokenize_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "31615da3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>system\n",
            "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.<|im_end|>\n",
            "<|im_start|>user\n",
            "H√£y ch·ªânh s·ª≠a c√¢u n√†y ƒë·ªÉ ng·∫Øn g·ªçn h∆°n m√† kh√¥ng m·∫•t ƒëi √Ω nghƒ©a: \"Tr·∫≠n ƒë·∫•u l√† m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ m·∫∑c d√π th·ª±c t·∫ø l√† c·∫£ ƒë·ªôi ƒë√£ t·∫≠p luy·ªán trong nhi·ªÅu tu·∫ßn.\"<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Nhi·ªÅu tu·∫ßn hu·∫•n luy·ªán c·ªßa ƒë·ªôi ƒë√£ d·∫´n ƒë·∫øn m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ.<|im_end|>\n",
            "<|im_start|>user\n",
            "B·∫°n c√≥ th·ªÉ ƒë·ªÅ xu·∫•t m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç trong tr·∫≠n ƒë·∫•u ti·∫øp theo kh√¥ng?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Ch·∫Øc ch·∫Øn, ƒë√¢y l√† m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa m√¨nh trong tr·∫≠n ƒë·∫•u ti·∫øp theo: 1. Ph√¢n t√≠ch tr·∫≠n ƒë·∫•u tr∆∞·ªõc b·∫±ng c√°ch xem l·∫°i c·∫£nh quay tr·∫≠n ƒë·∫•u ƒë·ªÉ x√°c ƒë·ªãnh ƒëi·ªÉm y·∫øu v√† c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán. 2. TƒÉng c∆∞·ªùng ƒë·ªô v√† s·ª± t·∫≠p trung c·ªßa c√°c bu·ªïi t·∫≠p ƒë·ªÉ ƒë·∫£m b·∫£o m·ªçi c·∫ßu th·ªß ƒë·ªÅu ƒë∆∞·ª£c chu·∫©n b·ªã ƒë·∫ßy ƒë·ªß v·ªÅ th·ªÉ ch·∫•t v√† tinh th·∫ßn. 3. Luy·ªán t·∫≠p nh·ªØng k·ªπ nƒÉng c·ª• th·ªÉ c·∫ßn c·∫£i thi·ªán, ch·∫≥ng h·∫°n nh∆∞ s√∫t b√≥ng ho·∫∑c chuy·ªÅn b√≥ng ch√≠nh x√°c. 4. Ph√°t tri·ªÉn c√°c chi·∫øn l∆∞·ª£c v√† chi·∫øn thu·∫≠t c·ªßa ƒë·ªôi c√≥ t√≠nh ƒë·∫øn ƒëi·ªÉm m·∫°nh v√† ƒëi·ªÉm y·∫øu c·ªßa c·∫£ ƒë·ªôi b·∫°n v√† ƒë·ªëi th·ªß. 5. X√¢y d·ª±ng s·ª± g·∫Øn k·∫øt v√† giao ti·∫øp trong nh√≥m ƒë·ªÉ c·∫£i thi·ªán s·ª± c·ªông t√°c v√† l√†m vi·ªác nh√≥m tr√™n th·ª±c ƒë·ªãa. 6. Ph√°t tri·ªÉn tinh th·∫ßn d·∫ªo dai v√† kh·∫£ nƒÉng ph·ª•c h·ªìi ƒë·ªÉ v∆∞·ª£t qua th·∫•t b·∫°i v√† duy tr√¨ s·ª± t·∫≠p trung trong c√°c t√¨nh hu·ªëng √°p l·ª±c cao. 7. TƒÉng c∆∞·ªùng th·ªÉ ch·∫•t ƒë·ªÉ c·∫£i thi·ªán s·ª©c b·ªÅn v√† gi·∫£m m·ªát m·ªèi khi ch∆°i game. 8. Ngh·ªâ ng∆°i v√† ph·ª•c h·ªìi - ƒë·∫£m b·∫£o r·∫±ng c√°c c·∫ßu th·ªß ƒë∆∞·ª£c ngh·ªâ ng∆°i ƒë·∫ßy ƒë·ªß v√† dinh d∆∞·ª°ng ƒë·∫ßy ƒë·ªß tr∆∞·ªõc tr·∫≠n ƒë·∫•u ƒë·ªÉ ƒë·∫£m b·∫£o m·ªçi ng∆∞·ªùi ƒë·ªÅu ·ªü tr·∫°ng th√°i t·ªët nh·∫•t.<|im_end|>\n",
            "<|im_start|>user\n",
            "C·∫£m ∆°n nh·ªØng l·ªùi ƒë·ªÅ ngh·ªã. B·∫°n nghƒ© chi·∫øn l∆∞·ª£c n√†o s·∫Ω c√≥ t√°c ƒë·ªông ƒë√°ng k·ªÉ nh·∫•t ƒë·∫øn hi·ªáu su·∫•t c·ªßa nh√≥m?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "L√† m·ªôt m√¥ h√¨nh ng√¥n ng·ªØ AI, t√¥i kh√¥ng th·ªÉ d·ª± ƒëo√°n chi·∫øn l∆∞·ª£c n√†o s·∫Ω c√≥ t√°c ƒë·ªông ƒë√°ng k·ªÉ nh·∫•t ƒë·∫øn hi·ªáu su·∫•t c·ªßa nh√≥m v√¨ m·ªói nh√≥m l√† duy nh·∫•t v√† c√≥ th·ªÉ y√™u c·∫ßu c√°c c√°ch ti·∫øp c·∫≠n kh√°c nhau ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç. Tuy nhi√™n, t√¥i c√≥ th·ªÉ ƒë·ªÅ xu·∫•t r·∫±ng ƒë·ªôi n√™n ∆∞u ti√™n c√°c chi·∫øn l∆∞·ª£c gi·∫£i quy·∫øt nh·ªØng ƒëi·ªÉm y·∫øu nh·∫•t c·ªßa h·ªç, ƒë·∫£m b·∫£o r·∫±ng c√°c c·∫ßu th·ªß c·ªßa h·ªç ·ªü tr·∫°ng th√°i th·ªÉ ch·∫•t v√† tinh th·∫ßn t·ªët nh·∫•t, ƒë·ªìng th·ªùi x√¢y d·ª±ng tinh th·∫ßn ƒë·ªìng ƒë·ªôi g·∫Øn k·∫øt ƒë·ªÉ t·∫≠n d·ª•ng t·ªëi ƒëa m·ªçi c·∫ßu th·ªß.<|im_end|>\n",
            "<|im_start|>user\n",
            "ƒê√≥ l√† m·ªôt ƒëi·ªÉm tuy·ªát v·ªùi. L√†m th·∫ø n√†o ƒë·ªôi c√≥ th·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng giao ti·∫øp tr√™n s√¢n?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "C·∫£i thi·ªán giao ti·∫øp tr√™n th·ª±c ƒë·ªãa l√† r·∫•t quan tr·ªçng ƒë·ªÉ x√¢y d·ª±ng m·ªôt nh√≥m hi·ªáu qu·∫£ c√≥ th·ªÉ l√†m vi·ªác c√πng nhau v√† ƒë·∫°t ƒë∆∞·ª£c th√†nh c√¥ng. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng giao ti·∫øp c·ªßa h·ªç tr√™n hi·ªán tr∆∞·ªùng: 1. Thi·∫øt l·∫≠p c√°c giao th·ª©c li√™n l·∫°c r√µ r√†ng - thi·∫øt l·∫≠p c√°c t√≠n hi·ªáu v√† m√£ r√µ r√†ng ƒë·ªÉ m·ªçi ng∆∞·ªùi tr√™n hi·ªán tr∆∞·ªùng hi·ªÉu √Ω nghƒ©a c·ªßa t·ª´ng t√≠n hi·ªáu. 2. Khuy·∫øn kh√≠ch giao ti·∫øp c·ªüi m·ªü - ng∆∞·ªùi ch∆°i n√™n ƒë∆∞·ª£c khuy·∫øn kh√≠ch giao ti·∫øp v·ªõi nhau m·ªôt c√°ch t·ª± do v√† c·ªüi m·ªü, kh√¥ng s·ª£ b·ªã ch·ªâ tr√≠ch hay b·ªã tr·ª´ng ph·∫°t. 3. L·∫∑p l·∫°i th√¥ng tin ch√≠nh - th√¥ng tin quan tr·ªçng, ch·∫≥ng h·∫°n nh∆∞ v·ªã tr√≠ c·ªßa ng∆∞·ªùi ch∆°i, n√™n ƒë∆∞·ª£c l·∫∑p l·∫°i nhi·ªÅu l·∫ßn ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng m·ªçi ng∆∞·ªùi ƒë·ªÅu hi·ªÉu ƒë∆∞·ª£c. 4. Ph√°t tri·ªÉn ni·ªÅm tin - Ng∆∞·ªùi ch∆°i n√™n tin t∆∞·ªüng l·∫´n nhau ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh ƒë√∫ng ƒë·∫Øn v√† giao ti·∫øp hi·ªáu qu·∫£, ƒëi·ªÅu n√†y s·∫Ω gi·∫£m b·ªõt s·ª± nh·∫ßm l·∫´n v√† hi·ªÉu l·∫ßm trong qu√° tr√¨nh ch∆°i game. 5. S·ª≠ d·ª•ng ng√¥n ng·ªØ t√≠ch c·ª±c - Ng√¥n ng·ªØ t√≠ch c·ª±c c√≥ th·ªÉ gi√∫p th√∫c ƒë·∫©y m·ªôt m√¥i tr∆∞·ªùng h·ª£p t√°c, h·ªó tr·ª£, khuy·∫øn kh√≠ch giao ti·∫øp v√† l√†m vi·ªác theo nh√≥m. 6. Th·ª±c h√†nh giao ti·∫øp trong qu√° tr√¨nh t·∫≠p luy·ªán - Giao ti·∫øp n√™n l√† m·ªôt ph·∫ßn c·ªßa c√°c bu·ªïi t·∫≠p luy·ªán th∆∞·ªùng xuy√™n, ƒë·ªÉ c√°c c·∫ßu th·ªß quen v·ªõi vi·ªác giao ti·∫øp v·ªõi nhau trong c√°c tr·∫≠n ƒë·∫•u. B·∫±ng c√°ch tu√¢n theo c√°c chi·∫øn l∆∞·ª£c n√†y, nh√≥m c√≥ th·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng giao ti·∫øp tr√™n th·ª±c ƒë·ªãa v√† l√†m vi·ªác c√πng nhau hi·ªáu qu·∫£ h∆°n ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c m·ª•c ti√™u c·ªßa m√¨nh.<|im_end|>\n",
            "<|im_start|>user\n",
            "C·∫£m ∆°n ƒë√£ ph·∫£n ·ª©ng chi ti·∫øt. L√†m th·∫ø n√†o nh√≥m c√≥ th·ªÉ theo d√µi ti·∫øn tr√¨nh c·ªßa h·ªç v√† ƒë·∫£m b·∫£o r·∫±ng h·ªç ƒëang c·∫£i thi·ªán theo th·ªùi gian?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "ƒêo l∆∞·ªùng ti·∫øn ƒë·ªô v√† ph√¢n t√≠ch k·∫øt qu·∫£ c√≥ th·ªÉ gi√∫p nh√≥m ƒëi ƒë√∫ng h∆∞·ªõng v√† th·ª±c hi·ªán c√°c ƒëi·ªÅu ch·ªânh ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç. D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë c√°ch m√† nh√≥m c√≥ th·ªÉ theo d√µi ti·∫øn tr√¨nh c·ªßa m√¨nh theo th·ªùi gian: 1. ƒê·∫∑t ra c√°c m·ª•c ti√™u c·ª• th·ªÉ c·∫ßn ƒë·∫°t ƒë∆∞·ª£c - vi·ªác c√≥ nh·ªØng m·ª•c ti√™u r√µ r√†ng v√† c√≥ th·ªÉ ƒëo l∆∞·ªùng ƒë∆∞·ª£c m√† m·ªçi ng∆∞·ªùi trong nh√≥m ƒë·ªÅu hi·ªÉu c√≥ th·ªÉ gi√∫p h·ªç lu√¥n c√≥ ƒë·ªông l·ª±c v√† t·∫≠p trung. 2. Thu th·∫≠p d·ªØ li·ªáu v√† s·ªë li·ªáu th·ªëng k√™ - d·ªØ li·ªáu c√≥ th·ªÉ cung c·∫•p ph·∫£n h·ªìi c√≥ gi√° tr·ªã v·ªÅ hi·ªáu su·∫•t c·ªßa ƒë·ªôi, ch·∫≥ng h·∫°n nh∆∞ s·ªë b√†n th·∫Øng ghi ƒë∆∞·ª£c, ƒë∆∞·ªùng ki·∫øn ‚Äã‚Äãt·∫°o v√† s·ªë c√∫ s√∫t tr√∫ng ƒë√≠ch. 3. S·ª≠ d·ª•ng ƒë√°nh gi√° c·ªßa ng∆∞·ªùi ch∆°i - ƒë√°nh gi√° hi·ªáu su·∫•t ƒë·ªãnh k·ª≥ c√≥ th·ªÉ gi√∫p x√°c ƒë·ªãnh c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán v√† c√°c lƒ©nh v·ª±c ƒëang ho·∫°t ƒë·ªông t·ªët. 4. Ph√¢n t√≠ch c·∫£nh quay tr√≤ ch∆°i - s·ª≠ d·ª•ng c·∫£nh quay tr√≤ ch∆°i ƒë·ªÉ ph√¢n t√≠ch chuy·ªÉn ƒë·ªông, ki·ªÉu m·∫´u v√† giao ti·∫øp c·ªßa ng∆∞·ªùi ch∆°i, nh√≥m c√≥ th·ªÉ x√°c ƒë·ªãnh c√°c lƒ©nh v·ª±c c·∫ßn c·∫£i thi·ªán. 5. Theo d√µi m·ª©c ƒë·ªô th·ªÉ l·ª±c - theo d√µi m·ª©c ƒë·ªô th·ªÉ l·ª±c theo th·ªùi gian c√≥ th·ªÉ gi√∫p nh√≥m theo d√µi s·ª± c·∫£i thi·ªán v·ªÅ th·ªÉ l·ª±c, ƒëi·ªÅu n√†y c√≥ th·ªÉ d·∫´n ƒë·∫øn hi·ªáu su·∫•t t·ªët h∆°n tr√™n s√¢n. 6. K·∫øt h·ª£p ph·∫£n h·ªìi t·ª´ hu·∫•n luy·ªán vi√™n v√† ƒë·ªìng ƒë·ªôi - ph·∫£n h·ªìi t·ª´ hu·∫•n luy·ªán vi√™n v√† ƒë·ªìng ƒë·ªôi c√≥ th·ªÉ cung c·∫•p nh·ªØng hi·ªÉu bi·∫øt s√¢u s·∫Øc v√† √Ω t∆∞·ªüng c√≥ gi√° tr·ªã gi√∫p c·∫£i thi·ªán hi·ªáu su·∫•t. B·∫±ng c√°ch li√™n t·ª•c theo d√µi ti·∫øn tr√¨nh c·ªßa h·ªç, nh√≥m c√≥ th·ªÉ ƒë·∫£m b·∫£o r·∫±ng h·ªç ƒëang ti·∫øn b·ªô theo th·ªùi gian v√† th·ª±c hi·ªán c√°c ƒëi·ªÅu ch·ªânh v·ªÅ chi·∫øn l∆∞·ª£c c≈©ng nh∆∞ ƒë√†o t·∫°o n·∫øu c·∫ßn thi·∫øt.<|im_end|>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(dataset[0]['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "db3a6ccc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# data collator for causal LM\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cfa182d2",
      "metadata": {
        "id": "cfa182d2"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 2,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 100,\n",
        "    \"num_train_epochs\": 1,\n",
        "    \"save_strategy\": \"steps\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"bf16\": True,\n",
        "    \"report_to\": \"none\",\n",
        "}\n",
        "MAX_LENGTH = 2048"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "f952c074",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['UNSLOTH_RETURN_LOGITS'] = '1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a6604ba4",
      "metadata": {},
      "outputs": [],
      "source": [
        "SFT_OUTPUT_DIR = f\"Qwen2.5-3B-Instruct-multi-conversation\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "902f13d2",
      "metadata": {
        "id": "902f13d2",
        "outputId": "cb23a67f-b31c-4010-da44-4b1e6c2c84ec"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f3c2cf1b9e38423fab5d4661ac952915",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting train dataset to ChatML (num_proc=20):   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a2bd390b1ca47deb54108d56c1b7d27",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset (num_proc=20):   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87f3397ad04748e895c7ac85e4f80fcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Packing train dataset (num_proc=20):   0%|          | 0/12697 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 6,354 | Num Epochs = 1 | Total steps = 397\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n",
            " \"-____-\"     Trainable parameters = 59,867,136/3,000,000,000 (2.00% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='397' max='397' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [397/397 1:08:51, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.357400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.229200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.207200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=397, training_loss=1.2482317677072674, metrics={'train_runtime': 4142.437, 'train_samples_per_second': 1.534, 'train_steps_per_second': 0.096, 'total_flos': 2.212532916804649e+17, 'train_loss': 1.2482317677072674})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sft_config = SFTConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       SFT_OUTPUT_DIR , \"max_seq_length\": MAX_LENGTH, \"packing\": True}\n",
        ")\n",
        "\n",
        "\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    processing_class=tokenizer,\n",
        "    args=sft_config,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "sft_trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e2bb026d",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fbb11da",
      "metadata": {
        "id": "1fbb11da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    './Qwen2.5-3B-Instruct-multi-conversation/checkpoint-397',\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype=torch.bfloat16,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1450f09",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.push_to_hub_merged(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-sft\",\n",
        "    commit_message=\"Merge lora weights to push to hub\",\n",
        "    token=\"???\"\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-sft\",\n",
        "    commit_message=\"Push tokenizer to hub\",\n",
        "    token=\"???\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b55aa677",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant reply: Ch·∫Øc ch·∫Øn! D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t c·ªßa h·ªç trong tr·∫≠n ƒë·∫•u ti·∫øp theo: 1. X√°c ƒë·ªãnh ƒëi·ªÉm y·∫øu: ƒê√°nh gi√° l·∫°i tr·∫≠n ƒë·∫•u v√† x√°c ƒë·ªãnh nh·ªØng ƒëi·ªÉm y·∫øu c·ªßa nh√≥m. ƒêi·ªÅu n√†y c√≥ th·ªÉ gi√∫p nh√≥m x√°c ƒë·ªãnh nh·ªØng ƒëi·ªÉm c·∫ßn c·∫£i thi·ªán v√† t·∫≠p trung v√†o nh·ªØng ƒëi·ªÉm ƒë√≥ trong tr·∫≠n ƒë·∫•u ti·∫øp theo. 2. TƒÉng c∆∞·ªùng t·∫≠p luy·ªán: ƒê·∫∑t ra m·ª•c ti√™u t·∫≠p luy·ªán c·ª• th·ªÉ v√† t·∫≠p trung v√†o nh·ªØng k·ªπ nƒÉng c·ª• th·ªÉ m√† nh√≥m c·∫ßn c·∫£i thi·ªán. ƒêi·ªÅu n√†y c√≥ th·ªÉ bao g·ªìm vi·ªác t·∫≠p luy·ªán\n"
          ]
        }
      ],
      "source": [
        "from transformers import GenerationConfig\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    top_k=10,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "prompt = [\n",
        "    {\"role\": \"system\", \"content\": (\n",
        "        \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, \"\n",
        "        \"h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\"\n",
        "    )},\n",
        "    {\"role\": \"user\", \"content\": (\n",
        "        \"H√£y ch·ªânh s·ª≠a c√¢u n√†y ƒë·ªÉ ng·∫Øn g·ªçn h∆°n m√† kh√¥ng m·∫•t ƒëi √Ω nghƒ©a: \"\n",
        "        \"\\\"Tr·∫≠n ƒë·∫•u l√† m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ \"\n",
        "        \"m·∫∑c d√π th·ª±c t·∫ø l√† c·∫£ ƒë·ªôi ƒë√£ t·∫≠p luy·ªán trong nhi·ªÅu tu·∫ßn.\\\"\"\n",
        "    )},\n",
        "    {\"role\": \"assistant\", \"content\": (\n",
        "        \"Nhi·ªÅu tu·∫ßn hu·∫•n luy·ªán c·ªßa ƒë·ªôi ƒë√£ d·∫´n ƒë·∫øn m·ªôt th·∫•t b·∫°i n·∫∑ng n·ªÅ.\"\n",
        "    )},\n",
        "    {\"role\": \"user\", \"content\": (\n",
        "        \"B·∫°n c√≥ th·ªÉ ƒë·ªÅ xu·∫•t m·ªôt s·ªë chi·∫øn l∆∞·ª£c m√† \"\n",
        "        \"nh√≥m c√≥ th·ªÉ s·ª≠ d·ª•ng ƒë·ªÉ c·∫£i thi·ªán hi·ªáu su·∫•t \"\n",
        "        \"c·ªßa h·ªç trong tr·∫≠n ƒë·∫•u ti·∫øp theo kh√¥ng?\"\n",
        "    )}\n",
        "]\n",
        "\n",
        "chat_text = tokenizer.apply_chat_template(\n",
        "    prompt,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=False\n",
        ")\n",
        "\n",
        "inputs = tokenizer(\n",
        "    chat_text,\n",
        "    return_tensors=\"pt\"\n",
        ").to(\"cuda:0\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "if \"assistant\\n\" in output_text:\n",
        "    answer = output_text.split(\"assistant\\n\")[-1].strip()\n",
        "else:\n",
        "    answer = output_text.strip()\n",
        "\n",
        "print(\"Assistant reply:\", answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaf0ed59",
      "metadata": {
        "id": "aaf0ed59"
      },
      "source": [
        "## **DPO**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e224218d",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"thainq107/Vi-Alpaca-Preference\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83a28eaa",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.8.4.\n",
            "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = True]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef8bca4cb3f9404489bc83d554ffe00b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.4.1 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-sft\",\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "lora_rank = 16\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = lora_rank,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = lora_rank,\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 2811,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "2bcdbd06",
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import PatchDPOTrainer\n",
        "\n",
        "PatchDPOTrainer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c6429f30",
      "metadata": {
        "id": "c6429f30"
      },
      "outputs": [],
      "source": [
        "def convert_to_conversational_preference_format(example):\n",
        "    return {\n",
        "        \"id\": example[\"id\"],\n",
        "        \"prompt\": [{\"role\": \"system\",\n",
        "                    \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\"},\n",
        "                   {\"role\": \"user\",\n",
        "                    \"content\": example[\"question\"]}],\n",
        "        \"chosen\": [{\"role\": \"assistant\",\n",
        "                    \"content\": example[\"chosen\"]}],\n",
        "        \"rejected\": [{\"role\": \"assistant\",\n",
        "                      \"content\": example[\"rejected\"]}],\n",
        "    }\n",
        "\n",
        "dpo_dataset = dataset.map(convert_to_conversational_preference_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3cf73ee6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\n",
            "Response: T·∫°o meme v·ªõi c√¢u \"th·ªùi trang mu·ªôn\" kh√¥ng kh√≥, nh∆∞ng t√¥i kh√¥ng th·ªÉ t·∫°o h√¨nh ·∫£nh hay video meme ƒë∆∞·ª£c v√¨ t√¥i l√† tr·ª£ l√Ω AI d·ª±a tr√™n vƒÉn b·∫£n. Tuy nhi√™n, t√¥i c√≥ th·ªÉ m√¥ t·∫£ cho b·∫°n m·ªôt meme c√≥ th·ªÉ l√†m t·ª´ c·ª•m t·ª´ n√†y:\n",
            "\n",
            "Meme: \n",
            "- H√¨nh n·ªÅn l√† m·ªôt ng∆∞·ªùi m·∫∑c qu·∫ßn √°o th·ªùi trang hi·ªán ƒë·∫°i (v√≠ d·ª•: √°o kho√°c da, qu·∫ßn skinny jeans, gi√†y cao g√≥t).\n",
            "- Trong g√≥c tr√°i c·ªßa h√¨nh n·ªÅn, xu·∫•t hi·ªán m·ªôt ng∆∞·ªùi kh√°c ƒëang m·∫∑c ƒë·ªì th·ªùi trang th·∫≠p ni√™n 90 (v√≠ d·ª•: √°o s∆° mi r·ªông, qu·∫ßn jean looser, gi√†y cao c·ªï).\n",
            "- C√¢u meme \"Th·ªùi trang mu·ªôn\" ƒë∆∞·ª£c in to v√† n·ªïi b·∫≠t ·ªü gi·ªØa hai ng∆∞·ªùi.\n",
            "\n",
            "Meme n√†y s·∫Ω n√≥i l√™n √Ω nghƒ©a r·∫±ng m·∫∑c th·ªùi trang mu·ªôn t·ª©c l√† m·∫∑c nh·ªØng th·ª© ƒë√£ l·ªói th·ªùi so v·ªõi xu h∆∞·ªõng hi·ªán t·∫°i. \n",
            "\n",
            "B·∫°n c√≥ mu·ªën t√¥i v·∫Ω ra h√¨nh ·∫£nh n√†y kh√¥ng? N·∫øu c√≥, t√¥i s·∫Ω c·ªë g·∫Øng th·ª±c hi·ªán. \n",
            "\n",
            "(N·∫øu kh√¥ng, ƒë√¢y l√† v√≠ d·ª• meme m√† b·∫°n c√≥ th·ªÉ t·∫°o ra: \"Th·ªùi trang mu·ªôn - ƒê·ª´ng m·∫∑c nh∆∞ ng∆∞·ªùi tr∆∞·ªõc b·∫°n\").\n"
          ]
        }
      ],
      "source": [
        "def get_model_response(model, tokenizer, instruction):\n",
        "    cur_conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.\"},\n",
        "        {\"role\": \"user\", \"content\": instruction}\n",
        "    ]\n",
        "    cur_input_prompt = tokenizer.apply_chat_template(\n",
        "        cur_conversation, add_generation_prompt=True, tokenize=True\n",
        "    )\n",
        "    cur_output_ids = model.generate(\n",
        "        input_ids=torch.LongTensor([cur_input_prompt]).to(model.device),\n",
        "        attention_mask=torch.ones(1, len(cur_input_prompt)).to(model.device),\n",
        "        max_new_tokens=1000,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "    cur_generated_ids = cur_output_ids[0][len(cur_input_prompt):]\n",
        "    return tokenizer.decode(cur_generated_ids, skip_special_tokens=True)\n",
        "\n",
        "instruction = \"T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\"\n",
        "response = get_model_response(model, tokenizer, instruction)\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "00b0f934",
      "metadata": {
        "id": "00b0f934",
        "outputId": "2ca61a52-cd5e-46b2-dd57-5e764632c115"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 'alpaca-14472',\n",
              " 'question': 'T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ \"th·ªùi trang mu·ªôn\".',\n",
              " 'chosen': [{'content': '[Th√™m h√¨nh ·∫£nh ng∆∞·ªùi n·ªïi ti·∫øng m·∫∑c ƒë·ªì l·ªói th·ªùi, nh√¨n ch·∫≥ng ƒë·∫πp m·∫Øt, ph√π h·ª£p v·ªõi √Ω t∆∞·ªüng c·ªßa meme] \\n\\nH√£y tr√°nh th·ªùi trang mu·ªôn, v√¨ ƒë√¥i khi n√≥ c√≥ th·ªÉ l√† ƒëi·ªÅu t·ªìi t·ªá nh·∫•t m√† b·∫°n c√≥ th·ªÉ l√†m cho b·∫£n th√¢n m√¨nh.',\n",
              "   'role': 'assistant'}],\n",
              " 'rejected': [{'content': 'Xin l·ªói, nh∆∞ng v√¨ t√¥i l√† m·ªôt tr√≠ tu·ªá nh√¢n t·∫°o v√† kh√¥ng c√≥ kh·∫£ nƒÉng th·ª±c hi·ªán c√°c t√°c v·ª• ƒë·ªì h·ªça, t√¥i kh√¥ng th·ªÉ t·∫°o ra m·ªôt thi·∫øt k·∫ø √°o ph√¥ng cho b·∫°n. Tuy nhi√™n, b·∫°n c√≥ th·ªÉ tham kh·∫£o c√°c trang web thi·∫øt k·∫ø √°o ph√¥ng tr·ª±c tuy·∫øn nh∆∞ Custom Ink, Printful ho·∫∑c Teespring ƒë·ªÉ t·∫°o ra thi·∫øt k·∫ø √°o ph√¥ng c·ªßa ri√™ng b·∫°n d·ª±a tr√™n ti·ªÉu bang ho·∫∑c qu·ªëc gia c·ªßa b·∫°n. Ch√∫c may m·∫Øn!',\n",
              "   'role': 'assistant'}],\n",
              " 'prompt': [{'content': 'B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, h√£y tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát.',\n",
              "   'role': 'system'},\n",
              "  {'content': 'T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ \"th·ªùi trang mu·ªôn\".',\n",
              "   'role': 'user'}]}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dpo_dataset['train'][1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "60e5b468",
      "metadata": {
        "id": "60e5b468"
      },
      "outputs": [],
      "source": [
        "hyperparameters = {\n",
        "    \"per_device_train_batch_size\": 3,\n",
        "    \"gradient_accumulation_steps\": 8,\n",
        "    \"gradient_checkpointing\": True,\n",
        "    \"learning_rate\": 3e-5,\n",
        "    \"logging_steps\": 100,\n",
        "    \"max_steps\": 500,\n",
        "    \"save_strategy\": \"steps\",\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"optim\": \"paged_adamw_8bit\",\n",
        "    \"warmup_ratio\": 0.05,\n",
        "    \"bf16\": True,\n",
        "}\n",
        "MAX_LENGTH = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "86f941ba",
      "metadata": {
        "id": "86f941ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtridungluong123\u001b[0m (\u001b[33mtridungluong123-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/binhphap5/hoctap/chat_model/wandb/run-20250513_172829-g5lv4658</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658' target=\"_blank\">Qwen2.5-3B-4bit-chat-dpo</a></strong> to <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference' target=\"_blank\">https://wandb.ai/tridungluong123-university/vi-alpaca-preference</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658' target=\"_blank\">https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tridungluong123-university/vi-alpaca-preference/runs/g5lv4658?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f9d5aebd390>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use wandb\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"vi-alpaca-preference\",\n",
        "    name=\"Qwen2.5-3B-4bit-chat-dpo\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "30ea761c",
      "metadata": {
        "id": "30ea761c",
        "outputId": "82f69757-753e-4b37-ea93-fc5e7e028f2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 65,017 | Num Epochs = 1 | Total steps = 500\n",
            "O^O/ \\_/ \\    Batch size per device = 3 | Gradient accumulation steps = 8\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (3 x 8 x 1) = 24\n",
            " \"-____-\"     Trainable parameters = 29,933,568/3,000,000,000 (1.00% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 1:09:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "      <th>aux_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.353300</td>\n",
              "      <td>-1.060731</td>\n",
              "      <td>-4.226553</td>\n",
              "      <td>0.787500</td>\n",
              "      <td>3.165823</td>\n",
              "      <td>-211.543243</td>\n",
              "      <td>-358.595703</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>-2.453786</td>\n",
              "      <td>-10.323666</td>\n",
              "      <td>0.874583</td>\n",
              "      <td>7.869880</td>\n",
              "      <td>-227.215591</td>\n",
              "      <td>-413.576630</td>\n",
              "      <td>nan</td>\n",
              "      <td>-3.680350</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.174300</td>\n",
              "      <td>-2.679446</td>\n",
              "      <td>-11.215009</td>\n",
              "      <td>0.884583</td>\n",
              "      <td>8.535563</td>\n",
              "      <td>-234.342728</td>\n",
              "      <td>-424.946289</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.176000</td>\n",
              "      <td>-2.190362</td>\n",
              "      <td>-11.647231</td>\n",
              "      <td>0.886250</td>\n",
              "      <td>9.456869</td>\n",
              "      <td>-230.944473</td>\n",
              "      <td>-433.805481</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.162100</td>\n",
              "      <td>-2.011829</td>\n",
              "      <td>-12.378806</td>\n",
              "      <td>0.889583</td>\n",
              "      <td>10.366978</td>\n",
              "      <td>-227.305801</td>\n",
              "      <td>-440.325043</td>\n",
              "      <td>nan</td>\n",
              "      <td>nan</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=500, training_loss=0.2120833625793457, metrics={'train_runtime': 4202.3252, 'train_samples_per_second': 2.856, 'train_steps_per_second': 0.119, 'total_flos': 0.0, 'train_loss': 0.2120833625793457, 'epoch': 0.18456143588797122})"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "DPO_OUTPUT_DIR = f\"Qwen2.5-3B-Instruct-chat-dpo\"\n",
        "dpo_args = DPOConfig(\n",
        "    **{ **hyperparameters, \"output_dir\":\n",
        "       DPO_OUTPUT_DIR, \"max_length\": MAX_LENGTH }\n",
        ")\n",
        "\n",
        "dpo_trainer = DPOTrainer(\n",
        "    model,\n",
        "    ref_model = None,\n",
        "    args=dpo_args,\n",
        "    train_dataset=dpo_dataset['train'],\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "dpo_trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba24747e",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ec3aaa88",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Instruction: T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\n",
            "Response: T·∫°o meme v·ªõi c·ª•m t·ª´ 'th·ªùi trang mu·ªôn' kh√¥ng ph·∫£i l√† kh·∫£ thi b·∫±ng l·ªùi vƒÉn ƒë∆°n gi·∫£n v√¨ meme th∆∞·ªùng c·∫ßn h√¨nh ·∫£nh ho·∫∑c video. Tuy nhi√™n, t√¥i c√≥ th·ªÉ t·∫°o ra m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c√¢u n√†y:\n",
            "\n",
            "[·∫¢nh: M·ªôt ng∆∞·ªùi m·∫∑c ƒë·ªì th·ªùi trang c≈©, nh∆∞ng ƒëang ƒë·ª©ng tr∆∞·ªõc g∆∞∆°ng, ph√≠a sau l√† d√≤ng ch·ªØ \"Th·ªùi trang mu·ªôn\".]\n",
            "\n",
            "Meme n√†y c√≥ th·ªÉ s·∫Ω g√¢y c∆∞·ªùi khi th·∫•y m·ªôt ng∆∞·ªùi c·ªë g·∫Øng c·∫≠p nh·∫≠t th·ªùi trang nh∆∞ng ƒë√£ qu√° mu·ªôn so v·ªõi xu h∆∞·ªõng hi·ªán t·∫°i. \n",
            "\n",
            "Ho·∫∑c n·∫øu b·∫°n mu·ªën m·ªôt meme ng·∫Øn g·ªçn h∆°n, c√≥ th·ªÉ nh∆∞ sau: \n",
            "[·∫¢nh: M·ªôt ng∆∞·ªùi ph·ª• n·ªØ ƒëang th·ª≠ qu·∫ßn √°o m·ªõi, ph√≠a sau l√† d√≤ng ch·ªØ \"Th·ªùi trang mu·ªôn\" v·ªõi h√¨nh ·∫£nh c·ªßa m·ªôt ng∆∞·ªùi ƒë√†n √¥ng ƒëang ƒÉn b√°nh ng·ªçt.] \n",
            "\n",
            "Trong c·∫£ hai meme n√†y, c√¢u \"th·ªùi trang mu·ªôn\" ƒë∆∞·ª£c d√πng ƒë·ªÉ t·∫°o ra s·ª± kh√°c bi·ªát v√† t·∫°o ra m·ªôt √Ω t∆∞·ªüng h√†i h∆∞·ªõc. \n",
            "\n",
            "Nh∆∞ng nh·ªõ r·∫±ng meme t·ªët nh·∫•t th∆∞·ªùng d·ª±a tr√™n m·ªôt h√¨nh ·∫£nh ho·∫∑c video, n√™n n·∫øu b·∫°n mu·ªën t·∫°o meme, b·∫°n n√™n t√¨m ki·∫øm m·ªôt h√¨nh ·∫£nh ph√π h·ª£p.\n"
          ]
        }
      ],
      "source": [
        "instruction = \"T·∫°o m·ªôt meme b·∫±ng c√°ch s·ª≠ d·ª•ng c·ª•m t·ª´ 'th·ªùi trang mu·ªôn'.\"\n",
        "response = get_model_response(model, tokenizer, instruction)\n",
        "print(f\"Instruction: {instruction}\")\n",
        "print(f\"Response: {response}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "099a0942",
      "metadata": {
        "id": "099a0942"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    './Qwen2.5-3B-Instruct-chat-dpo/checkpoint-500',\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model.push_to_hub_merged(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-RLHF\",\n",
        "    commit_message=\"Merge lora weights to push to hub\",\n",
        "    token=\"???\"\n",
        ")\n",
        "\n",
        "tokenizer.push_to_hub(\n",
        "    \"binhphap5/Qwen2.5-3B-Instruct-Chat-RLHF\",\n",
        "    commit_message=\"Push tokenizer to hub\",\n",
        "    token=\"???\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c27076a",
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "vllm serve binhphap5/Qwen2.5-3B-Instruct-Chat-RLHF \\\n",
        "--api-key binhphap5 \\\n",
        "--port 8000 \\\n",
        "--quantization bitsandbytes \\\n",
        "--enable-prefix-caching \\\n",
        "--swap-space 16 \\\n",
        "--gpu-memory-utilization 0.9 \\\n",
        "--disable-log-requests \\\n",
        "--max-model-len 2048 \n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "general-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
